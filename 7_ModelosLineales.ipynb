{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analítica y Ciencia de Datos\n",
    "\n",
    "## CIDE - Otoño 2015\n",
    "\n",
    "### Modelos lineales de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Referencias:\n",
    "\n",
    "Las propiedades y supuestos detrás del Modelo Lineal de Regresión, la pueden encontrar en [Greene, \"Econometric Analysis\"](http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm).\n",
    "\n",
    "También véanse los capítulos 3 y 6 de [ISL](http://www-bcf.usc.edu/~gareth/ISL/getbook.html).\n",
    "\n",
    "O el capítulo 3 de [ESL](http://statweb.stanford.edu/~tibs/ElemStatLearn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mínimos Cuadrados Ordinarios (OLS)\n",
    "\n",
    "* Sean $y_{N \\times 1}$, y $X_{N\\times (p+1)}$ las variables dependiente y matriz de regresores, respectivamente, donde $X$ incluye $p$ regresores y un vector de unos.\n",
    "\n",
    "\n",
    "* El modelo lineal es \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "y &=& X\\beta + \\epsilon\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "* El estimador de OLS es el vector $\\hat{\\beta}$ que minimiza la *suma de residuos al cuadrado* (SSR):\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} SSR(\\beta) = (y-X\\beta)'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "* La solución se encuentra fácilmente:\n",
    "$$\n",
    "\\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "y la matriz de varianzas y covarianzas es:\n",
    "\n",
    "$$\n",
    "\\Sigma_{\\beta} = \\hat{\\sigma}^2(X'X)^{-1}\n",
    "$$\n",
    "\n",
    "donde $\\hat{\\sigma}^2$ es la varianza residual estimada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunas propiedades del estimador de OLS\n",
    "\n",
    "*  **Prop 1: El estimador OLS es insesgado**:\n",
    "\n",
    "$$\n",
    "E(\\hat{\\beta}) = \\beta\n",
    "$$\n",
    "\n",
    "(qué supuestos hay que hacer para probarlo?)\n",
    "\n",
    "*  **Prop 2: Teorema de Gauss-Markov**:\n",
    "> El estimador OLS es el estimador de menor varianza, dentro de la clase de estimadores lineales insesgados.\n",
    "\n",
    "*  **Prop 3: Los residuos estimados son ortogonales a la matriz de regresores:**\n",
    "\n",
    "$$\n",
    "X'e = 0\n",
    "$$\n",
    "\n",
    "(¿por qué?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Propiedades (cont)\n",
    "\n",
    "\n",
    "*  **Prop 4: Teorema de Frisch-Waugh-Lovell**\n",
    "\n",
    "> En una regresión con dos variables $y = \\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + \\epsilon$, el estimador del coeficiente $\\alpha_i$ se puede obtener en un proceso de dos etapas:\n",
    "\n",
    "> **Etapa 1**: Obtenga los residuos de una regresión de $y$ sobre $x_{-i}$, $r_{y,-i}$, y de una regresión de $x_i$ sobre $x_{-i}$, $r_{x_i, -i}$\n",
    "\n",
    "> **Etapa 2**: en la regresión $r_{y,-i} =  \\eta_0 + \\eta_1 r_{x_i, -i}$, el estimador OLS \n",
    "$$\\hat{\\eta}_1 = \\hat{\\alpha}_i$$\n",
    "\n",
    "El Teorema de FWL es más general que en este caso de dos variables (ver Greene, por ejemplo), y es muy útil para entender qué sucede en OLS: cada coeficiente estimado es el efecto *neto* de los otros efectos.  Este proceso de *neteo* o *partialling out*, limpia la estimación de cualquier efecto compartido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Primera versión de un algoritmo de Backfitting\n",
    "\n",
    "* El Teorema de FWL nos dice que cada coeficiente se obtiene limpiando $y$ y $x_k$ de los demás regresores $x_{-k}$, y haciendo una regresión entre los residuales correspondientes.\n",
    "\n",
    "\n",
    "* En la práctica uno no desea hacer esto: para cada coeficiente $\\beta_k$ hay que hacer 3 regresiones!\n",
    "\n",
    "\n",
    "* Pero nos agudiza la intuición.\n",
    "\n",
    "\n",
    "* Además nos permite introducir una primera versión de un algoritmo de **backfitting** que tiene varias ventajas:\n",
    "\n",
    "    1. Sirve para entender el algoritmo de backfitting que vamos a utilizar más adelante para estimar modelos aditivos generalizados (Generalized Additive Models, o GAMs).\n",
    "    \n",
    "    2. Tiene varios parecidos con algoritmos que no vamos a ver en este curso como el *Gibbs Sampler* utilizado en estadística bayesiana, y el *EM* o *expectation-maximization*.\n",
    "    \n",
    "    3. Incluso cuando $p>>N$, podemos estimar los coeficientes, es decir, incluso cuando tenemos más variables explicativas que obsevaciones, porque utiliza regresiones bivariadas únicamente.\n",
    "    \n",
    "* El pseudocódigo sigue a continuación, y en la tarea tendrán que mostrar que funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pseudocódigo del algoritmo de backfitting para OLS\n",
    "\n",
    "```\n",
    "Inicialícese: beta_0 = 0\n",
    "Inicialícese la distancia: dist = 1\n",
    "\n",
    "while dist>0.001:\n",
    "    # En cada iteracción se hacen P+1 regresión bivariadas:\n",
    "    for k in range(nvars):\n",
    "        # seleccionemos las P columnas excluyendo la k-ésima\n",
    "        ind_col = cols/k\n",
    "        xmat_k = x[ind_col]\n",
    "        beta_k = beta_actual[ind_col]\n",
    "        # limpiemos y de los efectos de estas P variables seleccionadas\n",
    "        y_k = y - xmat_k*beta_k\n",
    "        # La actual columna es\n",
    "        x_k = x[k]\n",
    "        # regresión entre y_k y x_k, y actualizamos este coeficiente:\n",
    "        beta_actual[k] = ols(y_k, x_k)\n",
    "    # Actualícese la distancia:\n",
    "    dist = dist(beta_actual,beta_anterior)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variables Dummy y No Linealidad:\n",
    "\n",
    "* Consideremos un modelo con una variable dummy como regresor:\n",
    "\n",
    "$$\n",
    "y_i  = \\alpha + \\beta D_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "con $D_i = 1$ si individuo $i$ pertenece al grupo, y $D_i = 0$ si no pertenece.\n",
    "\n",
    "\n",
    "* Es inmediato mostrar que:\n",
    "\n",
    "$$\n",
    "\\beta = E(y_i|D_i = 1) - E(y_i|D_i = 0)\n",
    "$$\n",
    "\n",
    "así que la contraparte muestral es \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\overline{y}_1 - \\overline{y}_0 \n",
    "$$\n",
    "donde $\\overline{y}_i = \\frac{1}{N_i}\\sum_{i \\in \\{1,0\\}} y_i $, y $N_i$ el número de observaciones en el grupo $i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las variables dummies nos permiten modelar relaciones no lineales\n",
    "\n",
    "* Por ejemplo, supongamos que queremos modelar la rentabilidad de un cliente en función de su antigüedad.\n",
    "\n",
    "* Qué alternativas tenemos?\n",
    "\n",
    "    1. **Modelo lineal**: \n",
    "    $$\n",
    "    r_i = \\alpha_0 + \\alpha_1 a_i + \\epsilon_i\n",
    "    $$\n",
    "\n",
    "    2. **Modelo cuadrático**: \n",
    "    $$\n",
    "    r_i = \\alpha_0 + \\alpha_1 a_i + \\alpha_2 a_i^2 +\\epsilon_i\n",
    "    $$\n",
    "\n",
    "    3. **Modelo polinomial de orden $K$**: \n",
    "    $$\n",
    "    r_i = \\sum_{k=0}^K \\alpha_k a_i^k +\\epsilon_i\n",
    "    $$\n",
    "\n",
    "\n",
    "* Alternativamente, podemos dividir el rango de antigüedad and percentiles \n",
    "$$\n",
    "r_i = \\sum_{q= 0}^Q \\alpha_q D_{qi} + \\epsilon_i\n",
    "$$\n",
    "con \n",
    "$$\n",
    "D_{qi} = \n",
    "\\begin{cases}\n",
    "1 & \\text{si $a_i \\in [p_{q-1}(a), p_q(a)]$}\\\\\n",
    "0 & \\text{en otro caso}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Si hacemos un grid de percentiles lo suficientemente denso, podremos aproximar cualquier función, así que este método es muy flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expansiones polinomiales:\n",
    "\n",
    "* Como vimos en la diapositiva anterior, un método no paramétrico consiste en aproximar la función $f(X)$ por medio de una expansión por polinomios.\n",
    "\n",
    "\n",
    "* Por ejemplo, si tenemos tres variables $x_1, x_2, x_3$, una expansión polinomial posible es:\n",
    "\n",
    "$$\n",
    "y = \\alpha_0 + \\sum_{k=1}^3 \\beta_k x_k + \\sum_{k=1}^3 \\theta_k x_k^2 + \\sum_{i\\neq j} \\eta_{ij} x_i x_j + \\epsilon_i\n",
    "$$\n",
    "\n",
    "\n",
    "* La aproximación puede utilizar polinomios e interacciones de distintos órdenes, y los parámetros se pueden estimar por OLS sin problema, mientras que el número de términos incluido sea menor que el número de observaciones $N>p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selección de variables\n",
    "\n",
    "\n",
    "* En general no sabemos cuáles variables deben ir incluidas en un análisis, y este problema puede ser importante cuando tenemos muchos regresores $p$.\n",
    "\n",
    "\n",
    "* Hay varios métodos de selección de variables que permiten:\n",
    "\n",
    "    1. **Mejorar la predicción:** por ejemplo, porque un estimador tiene poco sesgo pero mucha varianza.\n",
    "    \n",
    "    2. **Facilitar la interpretación:** al eliminar ciertas variables podemos concentrarnos únicamente en las más importantes.\n",
    "    \n",
    " \n",
    "* Hay tres familias de métodos que se pueden utilizar:\n",
    "\n",
    "    1. Selección de subconjuntos de variables\n",
    "    \n",
    "    2. Regularización o reducción de la magnitud de los coeficientes o *shrinkage*\n",
    "    \n",
    "    3. Reducción de dimensionalidad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selección de subconjuntos: *Best Subset Selection*\n",
    "\n",
    "* Si tenemos $p$ regresores, queremos estimar las $\\binom{p}{k}$ posibles combinaciones de subconjuntos de $k$ variables.\n",
    "\n",
    "\n",
    "* Es un método exhaustivo que puede ser computacionalmente ineficientes cuando $p$ es grande.\n",
    "\n",
    "\n",
    "* Veamos cómo funciona este método utilizando los datos de tarjeta de crédito que están disponibles en la página de [ISL](http://www-bcf.usc.edu/~gareth/ISL/data.html).\n",
    "\n",
    "\n",
    "* Los datos incluyen el saldo en tarjeta de crédito (balance), el ingreso (Income) y límite de crédito  (Limit), score crediticio (Rating), número de tarjetas (Cards), edad, educación, género, variable indicadora del estatus de estudiante del cliente, casado, y etnicidad, para 400 clientes.\n",
    "\n",
    "* Queremos estimar el efecto que tienen estos regresores sobre el saldo o balance de cada cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('datasets/Credit.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caucasian' 'Asian' 'African American']\n",
      "(400, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      45.218885\n",
       "1    4735.600000\n",
       "2       2.957500\n",
       "3      55.667500\n",
       "4      13.450000\n",
       "5       0.482500\n",
       "6       0.100000\n",
       "7       0.612500\n",
       "8       0.247500\n",
       "9       0.255000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primero organicemos los datos: las variables cuantitativas están bien.  Las cualitativas toca transformarlas\n",
    "N = data.shape[0]\n",
    "dum_hombre     = np.asarray(data.Gender == ' Male').reshape((N,1))\n",
    "dum_estudiante = np.asarray(data.Student == 'Yes').reshape((N,1))\n",
    "dum_casado     = np.asarray(data.Married == 'Yes').reshape((N,1))\n",
    "# Para etnicidad debemos excluir una variable que sirva como referencia:\n",
    "# tenemos tres categorías: vamos a excluir la tercera: \"African American\"\n",
    "print data.Ethnicity.unique()\n",
    "dum_ethnic = np.asarray(pd.get_dummies(data.Ethnicity).iloc[:,:-1])\n",
    "# incluyamos todas las variables:\n",
    "prexmat = np.concatenate((np.asarray(data.Income).reshape((N,1)),\n",
    "                         np.asarray(data.Limit).reshape((N,1)),\n",
    "                         np.asarray(data.Cards).reshape((N,1)),\n",
    "                         np.asarray(data.Age).reshape((N,1)),\n",
    "                         np.asarray(data.Education).reshape((N,1)),\n",
    "                          dum_hombre, dum_estudiante, dum_casado, dum_ethnic\n",
    "                         ),axis=1)\n",
    "saldo = np.asarray(data.Balance).reshape((N,1))\n",
    "print prexmat.shape\n",
    "# revisemos que todo quedó bien:\n",
    "pd.DataFrame(prexmat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 2)\n"
     ]
    }
   ],
   "source": [
    "# Para las posibles combinaciones vamos a usar el módulo itertools\n",
    "from itertools import combinations\n",
    "import sys\n",
    "sys.path.append('/Users/danielvaughan/Documents/Python Scripts')\n",
    "import olsdan as ols\n",
    "\n",
    "# probémoslo con conjuntos de 9 elementos\n",
    "list(combinations(np.arange(10),9))\n",
    "\n",
    "# vamos a guardar los resultados en un array, pero no se cuántas filas tenemos\n",
    "res_arr = np.zeros((1,2))\n",
    "res_arr[:,:] = np.nan\n",
    "for k in range(1,11):\n",
    "    # la siguiente es la lista de todos los regresores posibles\n",
    "    list_subsets = list(combinations(np.arange(10),k))\n",
    "    # tenemos que hacer un loop sobre todas estas posibilidades:\n",
    "    for s in range(len(list_subsets)):\n",
    "        # los regresores seleccionados:\n",
    "        ind_s = np.array(list_subsets[s])\n",
    "        xmat_subset = prexmat[:,ind_s]\n",
    "        # listos para estimar: pero incluyamos constante\n",
    "        olssk = ols.ols_dan(saldo, np.concatenate((np.ones((N,1)),xmat_subset),axis=1))\n",
    "        # guardemos los resultados: queremos: número de regresores, ssr\n",
    "        prelist = np.array([k, olssk.ssr()]).reshape((1,2))\n",
    "        # listos para anexar:\n",
    "        res_arr = np.concatenate((res_arr, prelist),axis=0)\n",
    "print res_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x109f27f50>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109b05690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ya tenemos todos los modelos:\n",
    "# Hagamos un gráfico:\n",
    "plt.scatter(res_arr[:,0],res_arr[:,1], s=6, color='b',alpha=0.5)\n",
    "plt.ylabel('Suma de Residuos al Cuadrado')\n",
    "plt.xlabel(u'Número de Regresores')\n",
    "# Encontremos la frontera: como en el libro:\n",
    "res_arr_df = pd.DataFrame(res_arr, columns=['Num_Pred','SSR'])\n",
    "front_ssr = res_arr_df.SSR.groupby(res_arr_df.Num_Pred).min()\n",
    "plt.plot(front_ssr.index, front_ssr.values, color='r')                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cómo seleccionamos el mejor modelo?\n",
    "\n",
    "* Una vez tenemos la frontera, podemos utilizar cuatro opciones para seleccionar:\n",
    "\n",
    "    1. **Cross-Validation**: minimizar el SSR promedio.\n",
    "    \n",
    "    2. Minimizar $C_p$: \n",
    "    $$\n",
    "    C_p = \\frac{1}{n}(SSR + 2d\\hat{\\sigma}_2)\n",
    "    $$\n",
    "    donde $d$ es el número de regresores\n",
    "    3. Minimizar el Criterio de Información de Akaike:\n",
    "    $$\n",
    "    AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d \\hat{\\sigma}_2)\n",
    "    $$\n",
    "    4. Minimizar el Criterio de Información de Bayesiano:\n",
    "    $$\n",
    "    BIC = \\frac{1}{n}(RSS + \\ln(n)d \\hat{\\sigma}_2)\n",
    "    $$\n",
    "    5. Maximizar el $\\overline{R}^2$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
