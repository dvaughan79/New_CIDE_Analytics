{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analítica y Ciencia de Datos\n",
    "\n",
    "## CIDE - Otoño 2015\n",
    "\n",
    "### Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Referencias\n",
    "\n",
    "Capítulo 4 de [ESL](http://web.stanford.edu/~hastie/ElemStatLearn/).\n",
    "\n",
    "Capítulo 4 de [ISL](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "Capítulos de Modelos de Elección Discreta de un texto econométrico (Greene, Wooldridge o Cameron y Trivedi). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introducción a los Modelos de Clasificación\n",
    "\n",
    "\n",
    "* En las notas [introductorias](6_Intro_MachineLearning.ipynb) mencionamos algunos de los conceptos que siguen a continuación.\n",
    "\n",
    "\n",
    "* A diferencia de los modelos de regresión, los modelos de clasificación tienen como objetivo estimar la probabilidad de pertenecer a una categoría $k = 1, \\cdots, K$.\n",
    "\n",
    "\n",
    "* La variable dependiente $y_i$ es una variable *categórica* que indica a qué categoría pertenece el individuo $i$.\n",
    "\n",
    "\n",
    "* Así, $y_i \\in \\{1,0\\}$ indica la pertenencia a una de dos categorías, y las etiquetas $1,0$ carecen de contenido numérico u ordinal, así que los podríamos haber permutado, o nombrado de la manera que queramos.\n",
    "\n",
    "\n",
    "* Ejemplo del mundo de los negocios abundan:\n",
    "\n",
    "    * ¿Es el cliente alguien que potencialmente no paga su tarjeta de crédito?\n",
    "    \n",
    "    * ¿Es un cliente *fiel*, o nos abandonará en el siguiente período?\n",
    "    \n",
    "    * ¿Si le ofrecemos el producto, el cliente lo aceptará?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errores de Clasificación\n",
    "\n",
    "* A diferencia de un modelo de regresión, la estructura no ordinal de la variable dependiente enriquece el análisis de errores que podemos hacer para evaluar un modelo.\n",
    "\n",
    "\n",
    "* En regresión teníamos que una métrica de ajuste o error de un modelo era la *suma de residuos al cuadrado*.\n",
    "\n",
    "\n",
    "* En el caso de modelos de clasificación esta estructura es más rica.\n",
    "\n",
    "\n",
    "* Podemos empezar con el **error de clasificación**:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N I[\\hat{y}_i \\neq y_i]\n",
    "$$\n",
    "\n",
    "donde $I[x]$ es una variable indicadora que toma el valor $1$ cuando $x$ es verdadera y $0$ en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Pero cómo clasificamos?\n",
    "\n",
    "* Todavía no está claro, pero eventualmente formularemos modelos de la probabilidad de pertenecer a una categoría. \n",
    "\n",
    "\n",
    "* El resultado del modelo es una probabilidad estimada $\\hat{p}_{ik}$ de que el individuo $i$ pertenezca a la categoría $k \\in 1, \\cdots, K$.\n",
    "\n",
    "\n",
    "* La probabilidad nos da una medida contínua de pertenecer a una categoría, pero nuestro objetivo es predicir la categoría.\n",
    "\n",
    "\n",
    "* Así, por ejemplo, podemos decir que el individuo $i$ pertenece a la categoría que maximiza la probabilidad.\n",
    "\n",
    "    * En el caso de dos categorías: la predicción es \n",
    "    $$\n",
    "    i \\hat{\\in} k \\iff \\hat{p}_{ik}>0.5\n",
    "    $$\n",
    "    \n",
    "   \n",
    "* Pero **noten** que el valor límite o *threshold* $0.5$ es arbitrario, y más adelante veremos que podemos utilizar cualquiera en el intervalo $[min(\\hat{p}_{i,k}), max(\\hat{p}_{i,k})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Matrices de Confusión\n",
    "\n",
    "\n",
    "* Veamos el caso más simple de dos categorías $y_i \\in \\{0,1\\}$.\n",
    "\n",
    "\n",
    "* Aunque el error de clasificación anterior permite tener una primera métrica, para nosotros puede ser más relevante clasificar mal a una categoría que a otra.\n",
    "\n",
    "* Para esto se crea una matriz de confusión, donde las filas denotan la clasificación real u observada y las columnas la clasificación que se obtiene del modelo.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& \\bf \\hat{0} & \\bf \\hat{1} \\\\\n",
    "\\hline\n",
    "\\bf 0 & \\text{Verdaderos Positivos} & \\text{Falsos Negativos} \\\\\n",
    "\\hline\n",
    "\\bf 1 & \\text{Falsos Positivos} & \\text{Verdaderos Negativos} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "* Acá estamos siguiendo la tradición en esta literatura de denotar a la categoría $1$ como un negativo, y la categoría $0$ como positivo.\n",
    "\n",
    "\n",
    "* Lo importante es entender que con $K$ categorías tenemos la posibilidad de encontrar $K$ errores de clasificación distintos.\n",
    "\n",
    "* Hay dos métricas importantes que se obtienen de una matriz de confusión:\n",
    "\n",
    "    1. **Sensibilidad**: o tasa de verdaderos positivos\n",
    "    $$\n",
    "    \\text{sensibilidad} = \\frac{VP}{VP + FN}\n",
    "    $$\n",
    "    \n",
    "    2. **Especificidad**: o tasa de verdaderos negativos\n",
    "    $$\n",
    "    \\text{especificidad} = \\frac{VN}{VN + FP}\n",
    "    $$\n",
    "    \n",
    "* En el contexto de modelos de clasificación:\n",
    "\n",
    "    * **Error Tipo 1:** o falso positivo: $1-\\text{especificidad}$\n",
    "    * **Error Tipo 2:** o falso negativo: $1-\\text{sensibilidad}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Curvas ROC\n",
    "\n",
    " ![caption](figures/roc_curve.png)\n",
    " \n",
    "Figura 4.8 de ISL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Curvas ROC (cont.)\n",
    "\n",
    "\n",
    "* Las tasas de verdaderos positivos y falsos positivos son una buena medida del comportamiento de un modelo.\n",
    "\n",
    "\n",
    "* Sin embargo, dependen crucialmente del *threshold que utilicemos*.\n",
    "\n",
    "\n",
    "* La curva ROC permite mirar cómo cambian las tasas de verdadero y falsos positivos a medida que variamos los *thresholds* de clasificación.\n",
    "\n",
    "$$\n",
    "i \\hat{\\in} 0 \\iff \\hat{p}_{i0} > \\tau\n",
    "$$\n",
    "\n",
    "\n",
    "* Así, si $\\tau = 1$ la matriz de confusión es\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& \\bf \\hat{0} & \\bf \\hat{1} \\\\\n",
    "\\hline\n",
    "\\bf 0 & 0 & n_0 \\\\\n",
    "\\hline\n",
    "\\bf 1 & 0 & n_1 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "donde $n_i = \\#\\{\\text{$i$ en la muestra de entrenamiento}\\}$ y\n",
    "\n",
    "$$\n",
    "\\text{sensibilidad} = 0$, $1-\\text{especificidad} = 0\n",
    "$$\n",
    "\n",
    "obteniendo el punto en la esquina inferior izquierda de la curva ROC.\n",
    "\n",
    "* De la misma forma $\\tau=0$ implica que todas las observaciones se asignan a la categoría $0$ y obtenemos el punto superior derecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Curvas ROC (cont.)\n",
    "\n",
    "* ¿Qué pasa con la diagonal?\n",
    "\n",
    "* Los puntos de la diagonal tienen la característica que los errores de clasificación en una u otra categoría son exactamente iguales.\n",
    "\n",
    "* Corresponden a una asignación aleatoria (un \"volado\")\n",
    "\n",
    "    * Sirve como referencia o benchmark para medir a nuestros modelos.\n",
    "    \n",
    "\n",
    "* Intuitivamente quisieramos modelos que estén muy lejos de la diagonal.\n",
    "\n",
    "\n",
    "* Matemáticamente, quisieramos que el **área debajo de la curva** (AUC) sea lo mayor posible.\n",
    "\n",
    "\n",
    "* Noten el parecido con el coeficiente de Gini.  Todas las dificultades que se conocen para comparar distribuciones de ingreso mediante el coeficiente de Gini son válidas acá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costos directos, indirectos y de oportunidad\n",
    "\n",
    "* Una alternativa con modelos de clasificación es tener una medida del costo esperado de un modelo, es decir, tener en cuenta las asimetrías entre un clasificación correcta de cada alternativa.\n",
    "\n",
    "\n",
    "* Tomemos el caso de un modelo de *credit scoring*\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\begin{cases}\n",
    "1 & \\text{si cliente $i$ ha dejado de pagar sus créditos}\\\\\n",
    "0 & \\text{si cliente $i$ ha pagado sus créditos}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "* El score crediticio es una función de la probabilidad de que el cliente pague.\n",
    "\n",
    "\n",
    "* Así, un *menor* score sugiere que hay una mayor probabilidad de incumplimiento o de impago.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo Score Crediticio\n",
    "\n",
    "\n",
    "* Supongamos que la matriz de confusión para un modelo y *threshold* dados es\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& \\bf \\hat{0} & \\bf \\hat{1} \\\\\n",
    "\\hline\n",
    "\\bf 0 & n_{00} & n_{01} \\\\\n",
    "\\hline\n",
    "\\bf 1 & n_{10} & n_{11} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Miremos caso por caso:\n",
    "\n",
    "\n",
    "* **Caso $0\\hat{0}$**: cliente bueno y el modelo lo asigna correctamente.  Le damos una TdC, y nos deja una rentabilidad promedio de $r_{00} >0$.\n",
    "\n",
    "\n",
    "* **Caso $0\\hat{1}$**: cliente bueno pero el modelo lo clasifica como uno malo.  *No* le damos crédito, así que obtenemos una rentabilidad $r_{01} = 0$. $\\Rightarrow$ **Costo de oportunidad de mala clasificación!**\n",
    "\n",
    "\n",
    "* **Caso $1\\hat{0}$**: cliente malo, pero el modelo lo clasifica como si fuera bueno.  Le damos una TdC y no nos paga $r_{10}<0$.  $\\Rightarrow$ **costo directo de mala clasificación**\n",
    "\n",
    "\n",
    "* **Caso $1\\hat{1}$**: clientes malo y está bien clasificado.  No le damos una TdC, así que $r_{11} = 0$.\n",
    "\n",
    "\n",
    "##### ¿Cuál es la rentabilidad esperada que obtenemos del modelo?\n",
    "\n",
    "$$\n",
    "E(r|\\mathcal{M}) = \\sum_{ij} p_{ij}r_{ij}  = p_{00}r_{00} + p_{10}r_{10}\n",
    "$$\n",
    "\n",
    "donde $p_{ij} = n_{ij}/ \\sum_{kl} n_{kl}$\n",
    "\n",
    "##### ¿Cuál es la rentabilidad esperada que obtenemos del modelo, si incluimos el costo de oportunidad?\n",
    "$$\n",
    "E(r_{co}|\\mathcal{M}) = r_{00}(p_{00} - p_{01}) + p_{10} r_{10}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos lineales de probabilidad\n",
    "\n",
    "\n",
    "* Cuando hay sólo dos categorías, $y_i \\in {0,1}$, es posible estimar por OLS el modelo.\n",
    "\n",
    "$$\n",
    "y_i = x_i' \\beta + \\epsilon_i\n",
    "$$\n",
    "\n",
    "\n",
    "* ¿Por qué se llama un **modelo lineal de probabilidad**?\n",
    "\n",
    "\n",
    "* En general, queremos modelar \n",
    "$$\n",
    "Prob(y=1|X) = F(X,\\beta)\n",
    "$$\n",
    "\n",
    "Si $F(X,\\beta) = x\\beta$, entonces $E(y|x) = x\\beta$, y tenemos el modelo lineal.\n",
    "\n",
    "\n",
    "\n",
    "* Los modelos lineales de probabilidad tienen dos problemas:\n",
    "\n",
    "    1. Nada garantiza que $\\hat{y}_i \\in [0,1]$, afectando la interpretación del modelo.\n",
    "    \n",
    "    2. Heteroscedasticidad.\n",
    "    $$\n",
    "    Var(\\epsilon_i) = x_i'\\beta(1-x_i'\\beta)\n",
    "    $$\n",
    "\n",
    "\n",
    "* ¿Cómo se soluciona este problema?  En la **tarea** lo tendrán que hacer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos Probit\n",
    "\n",
    "\n",
    "* Una alternativa natural es asumir que $F(x_i,\\beta) = \\Phi(x_i'\\beta)$, donde $\\Phi$ es la función de distribución de una variable aleatoria normal estándar.\n",
    "\n",
    "\n",
    "* Este tipo de modelos es ampliamente utilizado en econometría, así que no los cubriremos en el curso.\n",
    "\n",
    "\n",
    "* Si miraremos a profundidad los modelos logísticos o Logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos Logit\n",
    "\n",
    "\n",
    "* Otra alternativa es usar \n",
    "$$\n",
    "F(x_i,\\beta) = \\Lambda(x_i'\\beta) = \\frac{e^{x_i'\\beta}}{1+e^{x_i'\\beta}}\n",
    "$$\n",
    "\n",
    "\n",
    "donde $\\Lambda(x_i'\\beta)$ denota a la función acumulativa logística.\n",
    "\n",
    "\n",
    "* ¿Cuál es el valor esperado de $y$?\n",
    "\n",
    "$$\n",
    "E(y|x) = 1 \\times F(x,\\beta) + 0 \\times (1-F(x,\\beta)) = F(x,\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "* Para encontrar los efectos marginales: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(y|x)}{\\partial x} = \\left[\\frac{d F(x,\\beta)}{d x'\\beta}\\right] \\beta = f(x,\\beta) \\beta\n",
    "$$\n",
    "\n",
    "donde $f()$ es la función de densidad.\n",
    "\n",
    "### Nota:\n",
    "##### A diferencia del modelo lineal de probabilidad donde los efectos marginales son constantes, los efectos marginales en los modelos Logit y Probit son funciones de $x$ y $\\beta$.   Generalmente se reportan *evalúandolos en las medias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Efectos marginales del Logit\n",
    "\n",
    "\n",
    "* En el caso del Logit es inmediato mostrar que:\n",
    "\n",
    "$$\n",
    "f(x,\\beta) = \\frac{e^{x'\\beta}}{(1+e^{x'\\beta})^2} = \\Lambda(x'\\beta)(1-\\Lambda(x'\\beta))\n",
    "$$\n",
    "\n",
    "* Así que para el caso de regresores contínuos podemos utilizar la fórmula de la diapósitiva anterior:\n",
    "\n",
    "$$\n",
    "\\text{Efectos Marginales} = \\frac{\\partial E(y|x)}{\\partial x_k} =  \\Lambda(\\overline{x}'\\beta)(1-\\Lambda(\\overline{x}'\\beta)) \\beta_k\n",
    "$$\n",
    "\n",
    "donde $\\overline{x} := [\\overline{x}_1, \\cdots, \\overline{x}_p]$.\n",
    "\n",
    "\n",
    "* En el caso de regresores categóricos (variables dummy), la derivada se reemplaza por el efecto marginal:\n",
    "\n",
    "$$\n",
    "\\text{Efectos Marginales regresor k} = Prob \\left(y=1 \\Bigg\\vert \\overline{x}_{(d)}, x_k=1 \\right) - Prob \\left(y=1 \\Bigg\\vert  \\overline{x}_{(d)}, x_k=0 \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Estimación de los modelos Logit\n",
    "\n",
    "* Para estimar el modelo es necesario maximizar la logverosimilitud\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{l}(\\beta) &=& \\sum_{i=1}^N \\log p_{g_i}(x_i;\\beta)\\\\\n",
    "&=& \\sum_{i=1}^N y_i \\ln \\Lambda(x_i,\\beta) + (1-y_i) \\ln (1-\\Lambda(x_i,\\beta))\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* Las condiciones de primer orden son:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{l}}{\\partial \\beta} = \\sum_{i} (y_i - \\Lambda_i)\\mathbf{x}_i = \\mathbf{0} = X'(y-\\hat{p})\n",
    "$$\n",
    "\n",
    "* El Hessiano es\n",
    "\n",
    "$$\n",
    "H = \\frac{\\partial^2 \\mathcal{l}}{\\partial \\beta \\partial \\beta'} = -\\sum_{i} \\Lambda_i (1-\\Lambda_i)\\mathbf{x}_i\\mathbf{x}_i' = X'WX \n",
    "$$\n",
    "\n",
    "con $W = diag(\\Lambda_i (1-\\Lambda_i))$\n",
    "\n",
    "* Se puede demostrar que la función de verosimilitud es globalmente cóncava, así que existen métodos numéricos iterativos eficientes para resolverlo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# El Método de Newton y Mínimos Cuadrados Ponderados Iterativamente (IRLS)\n",
    "\n",
    "* Supongamos que tenemos una estimación para la iteración $k$: $\\beta_{(k)}$\n",
    "\n",
    "\n",
    "* Vamos a hacer una aproximación de primer orden alrededor de esta estimación utilizando la condición de primer orden\n",
    "\n",
    "$$\n",
    "f(\\beta_{(k+1)}) \\approx f(\\beta_{(k)}) + \\nabla f(\\beta_{(k)})(\\beta_{(k+1)}-\\beta_{(k)})\n",
    "$$\n",
    "\n",
    "donde el gradiente $\\nabla$ de la condición de primer orden es el Hessiano\n",
    "\n",
    "\n",
    "* asumiendo que convergimos y el lado izquierdo es cero, despejando obtenemos la iteración de Newton\n",
    "\n",
    "$$\n",
    "\\beta_{(k+1)} = \\beta_{(k)} -  \\nabla f(\\beta_{(k)})^{-1}f(\\beta_{(k)})\n",
    "$$\n",
    "\n",
    "* Reemplazando, obtenemos\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_{(k+1)} &=& \\beta_{(k)} + (X'WX)^{-1}X'(y-\\hat{p})\\\\\n",
    "&=& (X'WX)^{-1}X'[WX\\beta_{(k)} + (y-\\hat{p})] \\\\\n",
    "&=& (X'WX)^{-1}X'W[X\\beta_{(k)} + W^{-1}(y-\\hat{p})] \\\\\n",
    "&=& (X'WX)^{-1}X'W z\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "con $z = X\\beta_{(k)} + W^{-1}(y-\\hat{p})$\n",
    "\n",
    "\n",
    "* Así que en cada iteración obtenemos una nueva estimación que es el resultado de estimar por mínimos cuadrados ponderados una regresión de $z$ en $X$ utilizando $W$ como matriz de ponderadores.\n",
    "\n",
    "\n",
    "* Este método converge muy rápidamente y tendrán que programarlo en la **tarea**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# La librería de optimización de Scipy\n",
    "\n",
    "* La ventaja del método de Newton que vimos en la diapósitiva anterior es que lo podemos programar directamente, y se puede ver que converge muy rápidamente.\n",
    "\n",
    "\n",
    "* Sin embargo es bueno que probemos algunas de las funciones que tiene Scipy para encontrar **mínimos**.\n",
    "\n",
    " ![caption](figures/scipy_optimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Simulemos un modelo\n",
    "\n",
    "* En la siguiente lámina vamos a simular un modelo.\n",
    "\n",
    "\n",
    "* Aunque existe una función para simular números aleatorios de una distribución logística, vamos a hacerlo utilizando el método que mencionamos hace algunas clases.\n",
    "\n",
    "\n",
    "* En el primer paso simulamos $u \\sim U[0,1]$.\n",
    "\n",
    "\n",
    "* Para obtener $x \\sim \\Lambda$, vamos a invertir la CDF correspondiente \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "u = \\frac{e^x}{1+e^x} \\iff x = \\ln(u) - \\ln(1-u)\n",
    "\\end{eqnarray*}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# Simulemos un modelo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(250679)\n",
    "N = 200\n",
    "\n",
    "# dos regresores independientes entre sí\n",
    "x0 = np.ones((N,1))\n",
    "x1 = 2*np.random.randn(N,1)\n",
    "x2 = np.random.randn(N,1)\n",
    "xmat = np.hstack((x0,x1,x2))\n",
    "beta_true = np.array([-2,3,5]).reshape((3,1))\n",
    "\n",
    "# vamos a simular residuos de un logit\n",
    "# Paso 1: simular N números aleatorios de una distribución U[0,1]\n",
    "# Paso 2: invertir la CDF de una v.a. logística\n",
    "u = np.random.rand(N,1)\n",
    "eps = np.log(u) - np.log(1-u)\n",
    "# variable dependiente latente\n",
    "y = np.dot(xmat,beta_true) + eps\n",
    "\n",
    "# necesitamos una variable dicótoma:\n",
    "d = 1*(y>0)\n",
    "# vamos la proporción\n",
    "\n",
    "print d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0011022073\n"
     ]
    }
   ],
   "source": [
    "# Vamos a optimizar una función de verosimilitud, así que generémosla\n",
    "\n",
    "def likeli_fn(betait, ymat,xmat):\n",
    "    '''\n",
    "    Función de log verisimilitud de un modelo Logit\n",
    "    El principal argumento es betait, que corresponde al beta_actual\n",
    "    '''\n",
    "    ymat = np.asarray(ymat)\n",
    "    xmat = np.asarray(xmat)\n",
    "    betait = np.reshape(np.asarray(betait),(len(betait),1))\n",
    "    # machine epsilon\n",
    "    eps = np.finfo(float).eps\n",
    "    # necesitamos la cdf\n",
    "    xbeta = np.dot(xmat,betait)\n",
    "    cdfi  = np.divide(np.exp(xbeta),1+np.exp(xbeta))\n",
    "    p1i   = np.multiply(ymat,np.log(cdfi))\n",
    "    p0i   = np.multiply(1-ymat,np.log(1-cdfi))\n",
    "    logli = np.add(p1i,p0i)\n",
    "    logli = -logli.sum()\n",
    "    return logli\n",
    "\n",
    "#probémosla evalúandola en los parámetros verdaderos\n",
    "likeli_true = likeli_fn(beta_true,d,xmat)\n",
    "print likeli_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2</td>\n",
       "      <td>0.394901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.152144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.239861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0 -2  0.394901\n",
       "1  3  0.152144\n",
       "2  5  0.239861"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para inicializar el algoritmo necesitamos un beta_inicial\n",
    "# Utilicemos OLS----> No está bien!\n",
    "import sys\n",
    "sys.path.append('/Users/danielvaughan/Documents/Python Scripts')\n",
    "import olsdan as ols\n",
    "\n",
    "olsinit = ols.ols_dan(d,xmat)\n",
    "beta_init = np.asarray(olsinit.betahat())\n",
    "pd.DataFrame(np.concatenate((beta_true,beta_init.reshape((3,1))),axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  status: 0\n",
       "    nfev: 185\n",
       " success: True\n",
       "     fun: 25.457879197541992\n",
       "       x: array([-2.09552344,  3.55413633,  5.92813581])\n",
       " message: 'Optimization terminated successfully.'\n",
       "     nit: 103"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probemos primero con Nelder-Mead\n",
    "nm_solution = optimize.minimize(likeli_fn,beta_init, args=(d,xmat),method='Nelder-Mead')\n",
    "nm_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ![caption](figures/optimize_minimize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Optimize: Nelder-Mead\n",
    "\n",
    "\n",
    "* Como podemos ver el algoritmo convergió a la solución en 103 iteraciones.\n",
    "\n",
    "\n",
    "* Nelder-Mead es un método numérico de optimización de *fuerza bruta*.\n",
    "\n",
    "\n",
    "* Si la función objetivo es *bonita*, funciona generalmente\n",
    "\n",
    "\n",
    "* Pero puede ser muy ineficiente: no utiliza información de la función.\n",
    "\n",
    "\n",
    " ![caption](figures/nelder_mead.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  status: 0\n",
       " success: True\n",
       "    njev: 42\n",
       "    nfev: 210\n",
       "     fun: nan\n",
       "       x: array([-39939.57482582,  95644.12187077,  39791.5611497 ])\n",
       " message: 'Optimization terminated successfully.'\n",
       "     jac: array([ nan,  nan,  nan])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probemos con el gradiente conjugado (conjugate gradient)\n",
    "cg_solution = optimize.minimize(likeli_fn,beta_init, args=(d,xmat),method='CG')\n",
    "cg_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   status: 2\n",
       "  success: False\n",
       "     njev: 42\n",
       "     nfev: 210\n",
       " hess_inv: array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])\n",
       "      fun: nan\n",
       "        x: array([-39939.57482582,  95644.12187077,  39791.5611497 ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "      jac: array([ nan,  nan,  nan])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probemos con el BFGS\n",
    "ncg_solution = optimize.minimize(likeli_fn,beta_init, args=(d,xmat),method='BFGS')\n",
    "ncg_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Dado que sabemos el gradiente y el hessiano, vamos a utilizar Newton-CG\n",
    "def gradient(betait,ymat,xmat):\n",
    "    '''\n",
    "    Gradiente de la función de log verosimilitud.  Ver notas\n",
    "    '''\n",
    "    ymat = np.asarray(ymat)\n",
    "    xmat = np.asarray(xmat)\n",
    "    nobs, nvar = xmat.shape\n",
    "    betait = np.reshape(np.asarray(betait),(len(betait),1))\n",
    "    # Calculemos la CDF\n",
    "    # necesitamos la cdf\n",
    "    xbeta = np.dot(xmat,betait)\n",
    "    cdfi  = np.divide(np.exp(xbeta),1+np.exp(xbeta))\n",
    "    # Usemos broadcasting\n",
    "    prevec = (ymat - cdfi).reshape((nobs,1))\n",
    "    grad = prevec*xmat\n",
    "    grad = -grad.sum(axis=0)\n",
    "    return grad\n",
    "\n",
    "def hessian(betait,ymat, xmat):\n",
    "    '''\n",
    "    Hessiano del modelo logit.  Ver notas.\n",
    "    '''\n",
    "    xmat = np.asarray(xmat)\n",
    "    nobs, nvar = xmat.shape\n",
    "    betait = np.reshape(np.asarray(betait),(len(betait),1))\n",
    "    # Necesitamos estimar la matriz W\n",
    "    # Primero: cdf\n",
    "    xbeta = np.dot(xmat,betait)\n",
    "    cdfi  = np.divide(np.exp(xbeta),1+np.exp(xbeta))\n",
    "    Pvec  = cdfi.reshape((nobs,1))\n",
    "    var_mat = np.multiply(Pvec,1-Pvec)\n",
    "    # No necesitamos la matriz diagonal.  Si N es grande nos quedamos sin memoria rápidamente\n",
    "    # En su lugar, vamos a usar broadcasting\n",
    "    prex = var_mat.reshape((nobs,1))*xmat\n",
    "    hess = np.dot(xmat.T,prex)\n",
    "    return hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 25.457879\n",
      "         Iterations: 10\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 20\n",
      "         Hessian evaluations: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.09552385,  3.55413405,  5.92810093])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prebeta = optimize.fmin_ncg(likeli_fn, x0=beta_init,\n",
    "                            fprime=gradient,\n",
    "                            fhess=hessian, args=(d,xmat),\n",
    "                            full_output=True, disp=True,\n",
    "                            retall=True)\n",
    "prebeta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sugerencias para optimizar\n",
    "\n",
    "\n",
    "* En estas notas quise introducir la librería de optimización.\n",
    "\n",
    "\n",
    "* Es una librería potente, y tener un ejemplo documentado de cómo usar diferentes métodos es práctico para intentar resolver otros problemas.\n",
    "\n",
    "\n",
    "* Sin embargo, como los ejemplos de arriba mostraron, antes de optimizar es **recomendable** (imperativo, mejor) tener un conocimiento de:\n",
    "\n",
    "    1. La función que vamos a optimizar (es cóncava, convexa, el problema es de maximización/minimización, tiene muchos puntos críticos).\n",
    "    2. El método que vamos a utilizar.\n",
    "    \n",
    "\n",
    "* Al final del día, estamos buscando un óptimo.  Y queremos estar seguros que el óptimo es efectivamente eso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularización y Logit: el Lasso\n",
    "\n",
    "\n",
    "* En las [láminas anteriores](7_ModelosLineales.ipynb) vimos el Lasso para el caso de modelos lineales.\n",
    "\n",
    "\n",
    "* Ahora veremos su implementación en el caso de modelos Logit.\n",
    "\n",
    "\n",
    "* Seguiremos de cerca la Sección 4.4.4 de ESL.\n",
    "\n",
    "\n",
    "* También se pueden ver [estas láminas de Trevor Hastie](http://web.stanford.edu/~hastie/TALKS/glmnet.pdf).\n",
    "\n",
    "\n",
    "* O el paper de [Friedman, Hastie y Tibshirani](http://core.ac.uk/download/files/153/6287975.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# El problema de optimización:\n",
    "\n",
    "\n",
    "* El objetivo es maximizar la función de logverosimilitud *penalizada*:\n",
    "\n",
    "$$\n",
    "\\max_{\\beta} \\sum_i^N y_i(x_i'\\beta) - \\log(1 + e^{x'\\beta}) - \\lambda \\sum_{i=1}^P |\\beta_i|\n",
    "$$\n",
    "\n",
    "\n",
    "* Igual que en el caso lineal, $\\lambda \\geq 0$ es la magnitud de penalización (elegida por nosotros), y penalizamos únicamente los coeficientes diferente de la constante.\n",
    "\n",
    "    * Por esta razón es conveniente estandarizar *la matriz $X$* antes de iniciar (y excluir la constante).\n",
    "    \n",
    "\n",
    "* Al igual que en el Lasso lineal, vamos a utilizar *coordinate descent* para encontrar los óptimos, así que se puede verificar fácilmente que la condición de primer orden es para la coordenada $j$:\n",
    "\n",
    "$$\n",
    "x_j'(y-p) = \\lambda \\cdot sign(\\beta_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pseudocódigo\n",
    "\n",
    "* La condición de primer orden es similar a la del caso lineal, aunque hay que recordar que las probabilidades estimadas dependen no linealmente de los parámetros a estimar.\n",
    "\n",
    "\n",
    "* Esta fue la razón para que utilizáramos una aproximación de Newton.\n",
    "\n",
    "\n",
    "* El pseudocódigo es así:\n",
    "\n",
    "\n",
    "    1. Fijamos un $\\lambda$\n",
    "    \n",
    "    2. Loop externo: Aproximación de Newton\n",
    "\n",
    "        * Este loop transforma el modelo en uno donde se estima Mínimos Cuadrados Ponderados Iterativos (IRLS)\n",
    "        \n",
    "    3. Loop interno (coordinate descent): iteramos sobre $k = 1, 2, \\cdots, P, 1, 2, \\cdots$ hasta converger\n",
    "    \n",
    "        * Utilizamos el Lasso lineal sobre las variables transformadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vamos a programarlo en Python\n",
    "\n",
    "\n",
    "* Para ver su funcionamiento en Python vamos a utilizar los datos [Enfermedades cardíacas en Sur África de ELS](http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data).\n",
    "\n",
    "\n",
    "* Así podremos comparar nuestros resultados con los del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\a3940004.EDIFICIOS\\\\Documents\\\\Python Scripts')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(u'D:\\\\Míos\\\\Clase CIDE\\\\Datasets\\\\els\\\\')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row.names</th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 160</td>\n",
       "      <td> 12.00</td>\n",
       "      <td> 5.73</td>\n",
       "      <td> 23.11</td>\n",
       "      <td> Present</td>\n",
       "      <td> 49</td>\n",
       "      <td> 25.30</td>\n",
       "      <td> 97.20</td>\n",
       "      <td> 52</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2</td>\n",
       "      <td> 144</td>\n",
       "      <td>  0.01</td>\n",
       "      <td> 4.41</td>\n",
       "      <td> 28.61</td>\n",
       "      <td>  Absent</td>\n",
       "      <td> 55</td>\n",
       "      <td> 28.87</td>\n",
       "      <td>  2.06</td>\n",
       "      <td> 63</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 3</td>\n",
       "      <td> 118</td>\n",
       "      <td>  0.08</td>\n",
       "      <td> 3.48</td>\n",
       "      <td> 32.28</td>\n",
       "      <td> Present</td>\n",
       "      <td> 52</td>\n",
       "      <td> 29.14</td>\n",
       "      <td>  3.81</td>\n",
       "      <td> 46</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4</td>\n",
       "      <td> 170</td>\n",
       "      <td>  7.50</td>\n",
       "      <td> 6.41</td>\n",
       "      <td> 38.03</td>\n",
       "      <td> Present</td>\n",
       "      <td> 51</td>\n",
       "      <td> 31.99</td>\n",
       "      <td> 24.26</td>\n",
       "      <td> 58</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 5</td>\n",
       "      <td> 134</td>\n",
       "      <td> 13.60</td>\n",
       "      <td> 3.50</td>\n",
       "      <td> 27.78</td>\n",
       "      <td> Present</td>\n",
       "      <td> 60</td>\n",
       "      <td> 25.99</td>\n",
       "      <td> 57.34</td>\n",
       "      <td> 49</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row.names  sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  \\\n",
       "0          1  160    12.00  5.73      23.11  Present     49    25.30    97.20   \n",
       "1          2  144     0.01  4.41      28.61   Absent     55    28.87     2.06   \n",
       "2          3  118     0.08  3.48      32.28  Present     52    29.14     3.81   \n",
       "3          4  170     7.50  6.41      38.03  Present     51    31.99    24.26   \n",
       "4          5  134    13.60  3.50      27.78  Present     60    25.99    57.34   \n",
       "\n",
       "   age  chd  \n",
       "0   52    1  \n",
       "1   63    1  \n",
       "2   46    0  \n",
       "3   58    1  \n",
       "4   49    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('heart_data.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Organicemos y transformemos los datos\n",
    "nobs = data.shape[0]\n",
    "# Nuestra variable dependiente: presencia de enfermedad cardiáca\n",
    "yy = data.chd.values.reshape((nobs,1))\n",
    "# Vamos a estandarizar todas las variables  independientes (incluida la dummy de Historia Familiar)\n",
    "labs_cont = np.array([u'sbp', u'tobacco', u'ldl', u'adiposity', u'typea', u'obesity', u'alcohol', u'age'])\n",
    "x_dum     = 1*(data.famhist=='Present').values.reshape((nobs,1))\n",
    "#xmat_cont = np.concatenate((data[labs_cont].values,x_dum),axis=1)\n",
    "xmat_cont = data[labs_cont].values\n",
    "vx = xmat_cont.shape[1]\n",
    "# Estandaricemos\n",
    "fullxmat = (xmat_cont - xmat_cont.mean(axis=0).reshape((1,vx)))/xmat_cont.std(axis=0).reshape((1,vx))\n",
    "fullxmat = np.concatenate((fullxmat, x_dum),axis=1)\n",
    "labs_x = np.array([u'sbp', u'tobacco', u'ldl', u'adiposity', u'typea', u'obesity', u'alcohol', u'age','famhist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Primera función: Lasso Lineal (igual que la que vimos antes)\n",
    "def linear_lasso(y,xmat,lambda_t):\n",
    "    '''\n",
    "    Estimate linear lasso.\n",
    "    I have two options:\n",
    "    1. Y and Xmat have already been standardized\n",
    "    2. Y and Xmat were demeaned, but not standardized\n",
    "    Note: demeaning is necessary because we don't want to penalize\n",
    "    '''\n",
    "    nobs, nvar = xmat.shape\n",
    "    # Inicialicemos el vector, la idea es que utilizamos un Lambda grande!\n",
    "    beta_actual = np.zeros((1,nvar))\n",
    "    dist = 1\n",
    "    counter = 0\n",
    "    maxiter = 10000\n",
    "    # loop externo: si no hay convergencia continúe\n",
    "    while dist>0.001 and counter<maxiter: \n",
    "        # Loop interno: ciclo con los regresores\n",
    "        beta_old = beta_actual.copy()\n",
    "        for k in range(nvar):\n",
    "            # Residuos parciales excluyendo k\n",
    "            inc_k    = np.setdiff1d(np.arange(nvar),np.array([k]))\n",
    "            xmat_k   = xmat[:,inc_k]\n",
    "            beta_k   = beta_actual[0,inc_k]\n",
    "            resids_k = y - np.dot(xmat_k, beta_k.reshape((nvar-1,1)))\n",
    "            #-------------------------------------------\n",
    "            # Simple OLS: is just $\\epsilon_{¬k}'x_k/N$: if Y,X are standardized\n",
    "            #-------------------------------------------\n",
    "            #beta_star_k = np.dot(resids_k.T,xmat[:,k].reshape((nobs,1)))/(1.0*nobs*x_2)\n",
    "            #-------------------------------------------\n",
    "            # Standard OLS: is just $\\epsilon_{¬k}'x_k/x_k'x_k$: if Y,X are not standardized\n",
    "            #-------------------------------------------\n",
    "            # NOTE: if x was standardized I get the same results, since \\sum_i x_i^2 = N s^2 = N\n",
    "            #-------------------------------------------\n",
    "            x_2      = np.dot(xmat[:,k].reshape((1,nobs)), xmat[:,k].reshape((nobs,1)))[0]\n",
    "            beta_star_k = np.dot(resids_k.T,xmat[:,k].reshape((nobs,1)))/x_2\n",
    "            # Soft-thresholding\n",
    "            beta_actual[0,k] = np.sign(beta_star_k)*np.max([0,np.abs(beta_star_k)-lambda_t])\n",
    "        # Actualicemos distancia:\n",
    "        dist = np.max(np.abs(beta_actual-beta_old))\n",
    "        counter +=1\n",
    "    # fin del loop\n",
    "    return beta_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>famhist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0.026255</td>\n",
       "      <td> 0.07517</td>\n",
       "      <td> 0.067735</td>\n",
       "      <td> 0.01518</td>\n",
       "      <td> 0.058259</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>-0.004307</td>\n",
       "      <td> 0.100568</td>\n",
       "      <td> 0.084649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sbp  tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "0  0.026255  0.07517  0.067735    0.01518  0.058259 -0.043407 -0.004307   \n",
       "\n",
       "        age   famhist  \n",
       "0  0.100568  0.084649  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probémosla\n",
    "pd.DataFrame(linear_lasso(yy,fullxmat, 0.001), columns=labs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Segunda Función: Logit Lasso\n",
    "def logit_lasso(ymat,xmat, lambda_t, beta_init):\n",
    "    '''\n",
    "    This function computes the Lasso for the Logit as in Friedman, et.al. (2010), \"Regularizatio Paths\"...\n",
    "    yy: binary dependent variable\n",
    "    xmat: regressors, excluding constant, so xmat must have been demeaned/standardized\n",
    "    lambda_t: current value of lambda\n",
    "    beta_init: user-provided initial values\n",
    "    '''\n",
    "    nobs, nvar = xmat.shape\n",
    "    # Initialize parameters\n",
    "    dist = 1\n",
    "    critval = 1e-4\n",
    "    counter = 0\n",
    "    MaxIters = 100\n",
    "    # outer loop: I need to get a Newton-Update of the IRLS for the logit\n",
    "    # I need to initialize beta_old\n",
    "    beta_new = beta_init.reshape((nvar,1))\n",
    "    #DistMat = np.zeros((MaxIters,1))\n",
    "    while dist>critval and counter<MaxIters:\n",
    "        beta_old = beta_new.copy().reshape((nvar,1))\n",
    "        # -------------------------------------------\n",
    "        # Given beta_old, I need to set up the linear model z = xbeta + w^{-1}(y-p), x,w\n",
    "        # -------------------------------------------\n",
    "        xbeta = np.dot(xmat,beta_old)\n",
    "        Pmat  = np.divide(np.exp(xbeta), 1+np.exp(xbeta))\n",
    "        PreW  = np.multiply(Pmat,1-Pmat)\n",
    "        # check if zeros\n",
    "        PreW[PreW<1e-4]   = 0.001\n",
    "        PreW[PreW>0.9999] = 0.99\n",
    "        # Y-p\n",
    "        Y_minusP = ymat - Pmat\n",
    "        # I don't want to use the diagonal matrix as it becomes prohibitely large with large N\n",
    "        # I can just use broadcasting:\n",
    "        w_inv  = 1.0/PreW\n",
    "        z      = xbeta + np.multiply(w_inv, Y_minusP)\n",
    "        #------------------------------\n",
    "        # Ready to transform the model:\n",
    "        #------------------------------\n",
    "        wmat = np.sqrt(PreW)\n",
    "        z_star = np.multiply(wmat,z)\n",
    "        x_star = wmat*fullxmat\n",
    "        # I need to demean x_star\n",
    "        #z_star = z_star - z_star.mean()\n",
    "        #x_star = \n",
    "        #--------------------------------------\n",
    "        # From here it's just the lasso on z_star and x_star\n",
    "        #--------------------------------------\n",
    "        beta_new = linear_lasso(z_star,x_star, lambda_t)\n",
    "        # Actualicemos distancia:\n",
    "        dist = np.max(np.abs(beta_new.reshape((nvar,1))-beta_old))\n",
    "        #DistMat[counter] = dist\n",
    "        #--------------------------------------\n",
    "        # update counter\n",
    "        #--------------------------------------\n",
    "        counter +=1\n",
    "    return beta_new, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>famhist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0.009872</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sbp  tobacco  ldl  adiposity  typea  obesity  alcohol       age  famhist\n",
       "0    0        0    0          0      0        0        0  0.009872        0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try it\n",
    "beta, count = logit_lasso(yy,fullxmat, 0.7, np.zeros((9,1)))\n",
    "print count\n",
    "pd.DataFrame(beta, columns = labs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Finalmente, escribamos una función para obtener todo el Path de coeficientes regularizados\n",
    "def lasso_logit_path(ymat,xmat,lambda_max, lambda_min= 0.0000001, G=10):\n",
    "    '''\n",
    "    This function computes the whole path of the Lasso for the Logit.\n",
    "    ymat: dependent variable (1/0)\n",
    "    xmat: independent variables (already standardized)\n",
    "    lambda_min: by default is something very small, close to zero\n",
    "    lambda_max: user must provide it, large enough for everything to be zero\n",
    "    G: number of grid points.  Default is 10\n",
    "    ----------------------------------------------------\n",
    "    Note: I'm using warm starts\n",
    "    The only thing that is interesting is that I start with \n",
    "    an \\beta_{00} = 0, and then \\beta_{0,k} = \\beta_{opt, k-1}\n",
    "    '''\n",
    "    nobs, nvar = xmat.shape\n",
    "    lambda_pth = np.linspace(lambda_min, lambda_max, G)\n",
    "    BetaPath = np.zeros((G,nvar))\n",
    "    for g in range(G):\n",
    "        if g == 0:\n",
    "            # I start from very large lambda's\n",
    "            beta_init = np.zeros((nvar,1))\n",
    "        else:\n",
    "            beta_init = beta_new.reshape((nvar,1))\n",
    "        lambda_it = lambda_pth[-1-g]\n",
    "        # Call my function \n",
    "        beta_new, count  = logit_lasso(ymat,xmat, lambda_it, beta_init)\n",
    "        # Save results\n",
    "        BetaPath[g,:] = beta_new.flatten()\n",
    "        \n",
    "    return BetaPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAbcAAAEbCAYAAABZScp+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzsnWeYZFW1ht9vAjCEIWdRBMlJchiBQxAQQTKKIDJIRlFR\n",
       "QEw1paBXuV5R9CKKgoJIBhEkCRyUcIkqIllFBQTJA0gYZr77Y51menp6eqq7q7q6a9b7PPV0hXP2\n",
       "XlVdtddZa68g2yRJkiRJJzGq3QIkSZIkSbNJ5ZYkSZJ0HKnckiRJko4jlVuSJEnScaRyS5IkSTqO\n",
       "VG5JkiRJx5HKLRlSJE2TtEK75egvkiZJOqvdcnQakk6V9MV2y5F0Hqncktki6VFJ/5H0kqQnJZ0h\n",
       "ab4GzislfWyQc68s6QJJT0t6QdIfJX1a0lB/dxtOCB3s+64+720Gen6raIVctg+3fUI1fiHpn80c\n",
       "P5lzSeWWNIKBnWwvAKwHbAA0crU9qAoBklYEbgP+DqxpeyFgL2B9YIHBjD0Qcfpx7GArI7gJY7SC\n",
       "4SpXksxEKrekX9h+ArgKWFPSQpIul/RvSc9J+pWkZQEknQhsDnyvsvi+222Y90p6SNLzkr7Xx3R1\n",
       "4Cbbn7X9VDX/Q7b3s/1iNc8HJP25GusGSat2nVxZGp+VdE8lw48lLSnpSkkvSrpW0kLVsctXLtOD\n",
       "JT0u6QlJn5mVYJI2kXRLNe8fJG3Z1/uWtJmkOyrr83ZJm/b3s+/r865eP0DSXyRNlvRXSR+unn+X\n",
       "pBuruZ+WdG63c5oh19ySTq4+t8clfVvSXN1eP7b6PB+TdFB317SkMyV9VdK8wJXAMtXnNlnSUv2V\n",
       "JUnewnbe8tbnDfgbsE11fzngXkLxLALsBswDzA+cD1zS7bwbgAN7jDUNuAwYX431b2D7Wcz7L+Cj\n",
       "fci1MvAysA0wGjgGeBgY003uW4DFgWWAp4C7gXWAuYHrgC9Xxy5fyfZzYBywZiVb1/ueBJxV3V8W\n",
       "eAbYoXq8bfV40d7ed/U5PQ/sS1xQfgh4Dlikj897616en+XnDcwHvAisVD1eEli9uv8L4Pjq/lzA\n",
       "Zk2W6yvV57xYdbsZ+Er12g7V/3G16nM9u/qcV6heP6PbsVsC/2z39z1vnXFLyy1pBAGXSnoe+B1Q\n",
       "Al+z/ZztS2y/Zvtl4GvEAtXz3J78l+3Jtv9JKIJ3z2LeRYmFcVZ8ELjc9nW2pwL/TSygm3U75hTb\n",
       "Tzsszt8Bt9r+o+3XgUuAdXuMWbf9qu17iYV3n17m3Q/4te2rAGz/BrgTeH+3Y7q/7/cDD9r+ue1p\n",
       "ts8FHgB27uO9zUQDn/c0YC1J42w/Zfu+6vk3gOUlLWv7Ddu3NFMu4MOEgnrG9jPEhc9Hqtf2Bn5i\n",
       "+37brwK1Xs5Xj79JMmhSuSWNYGAX2wvbXt72x22/LmleSadV7r8XgRuBBSWpx7k9ebLb/f8QVkhv\n",
       "PEtYXLNiaeAfb01kG/gnYVl18VS3+6/2ePxaL3N3D2j4xyzmfwewV+WSfL5S+hOA7m607u97me5y\n",
       "Vvy9h5yzpa/P2/YrhLI/DHiicl+uUp16LKE4bpd0r6SJzZSrGufv3R53/9yWZsbP9LF+jp0kAyKV\n",
       "WzIYPkO4BjeyvSBhRYjpV+CDDT74DbBHH68/QSgaACqluhzweB/nzM46eHuP+72N9Q/CRblwt9sC\n",
       "tr9Zvd7zfT/eXc6Kd9D/hb7Pz9v2Nba3I5TsA8CPquefsn2I7WWBQ4H/rYJ1miXXE4Rbt4vun9u/\n",
       "iP9JF93vd+Eef5Nk0KRySwbD/IQ19KKkRZjZ5fQUsOJsxuhL2dSAzSR9U9KS8FZwxFmSxhN7Tu+X\n",
       "tLWkscTi/xqx/zNQvihpnKQ1gAOA83o55mxgZ0nbSRotaZ4qjL3L4un5vn8NrCxpH0ljJH0QWBW4\n",
       "vA855qrG7bqNoY/PW9ISknZRpGhMAV4Bplav7SXpbdWhLxBKZCoRwNEMuX5RfW6LSVoM+HL1GUH8\n",
       "jyZKWrUKGvlSj/G6Xww9BSxa/W+TZFCkcksGw8nEHtczhEK5khmvvr8D7FlF9p08izFmGV5u+6/A\n",
       "poRV8GdJLwAXAncAL9t+iNj/OgV4mthD2tn2m33I7B73e859I/AIYTWeVO2nzXCs7ceAXYDPE0En\n",
       "/yAUa9ciPcP7tv0csFN1zDPAZ4nUiuf6kPPXhMu26/Zl+v68RwGfJiymZ4mIzcOr1zYA/k/SS8Av\n",
       "gaNsP2r72SbJdQKx53hPdbuzeo5qX/K7xN7qQ8Ct1TivV3+7f64PEIryr9Vnl9GSyYBRbFMkyZyN\n",
       "pOWBvxKRltPaK03nImk14E/AXPk5J60kLbckSVqKpN2qXLiFgW8Al6ViS1pNKrckmU66MVrDIcR+\n",
       "2iPEfuDhfR+eJIMn3ZJJkiRJx5GWW5IkSdJxjGnn5JJ2ICLARgOn2/5GL8cUwLeBscAztoser6fp\n",
       "mSRJMgBsd2xVmLa5JSWNBh4k6vI9ToR372P7/m7HLETUqdve9mOSFqvK+3QfxyPhHyRpku1J7ZZj\n",
       "dqSczSXlbC4jQc6RICOMnLVzoLTTLbkR8EiVbzMFOJfIHerOh4GLqrwieiq2JEmSJOmNdiq3ZZm5\n",
       "5lzPmnYrAYsoWpncKekjJEmSJANGQhKbt1uOVtPOPbdG/KFjieaY2wDzArdK+j/bD3c/SNKkbg9L\n",
       "22WzhGwiZbsFaJCy3QI0SNluARqkbLcADVK2W4AGKdstQAOU7RagN6T5t4GNDod1NoWx87RbnlbT\n",
       "zj23TYBJtneoHh8PTOseVCLpOGBcl/9a0unAVbYv7HZMR/uNkyRJBoPEeOAg4JPAo8C3gMtBUzt5\n",
       "7WynW/JOYCVFB+S5iHYdl/U45pfAe6ritPMCGwP3kSRJkvSJxHISJxFNZjcC9rTZ0uYym46vENM2\n",
       "5VYVt/04cDWhsM6zfb+kQyUdWh3zAHAVUYz1NuBH3RowJkmSJD2QWE/i58AfiDV+PZsP2dwx+3N1\n",
       "SRXfcK+kg6vnPibpQUm3SfqRpFOq5xeXdKGk26vbZn2PPrSM+Aol6ZZMkmROR2IUsCPR4WFFohPD\n",
       "j2xenPU5M6+dkha2/bykccDtwPZEOta6wMvA9cAfbB8l6Rzg+7ZvlvR2Ysto9Va8v4HQ1iTuJEmS\n",
       "ZOBIjAM+AhxN9PD7FnCBzZQBDvlJSbtW95erxi5tvxDz6QKiYS5EjvJq0SMYgAUkzWv7PwOcu6mk\n",
       "ckuSJBlhSCwBHEEUob4dOAy40R548e+qGtQ2wCa2X5N0A9HRfbXuhzE90l3AxrbfGOicrSRrSyZJ\n",
       "kowQJFaV+CFR3WlpYEubnW3KwSi2ivHA85ViWxXYBJgP2FLSQlXX9T26HX8NcNR02fTuQc7fVFK5\n",
       "JUmSDGOqpOutJC4nOsU/Dqxsc6jNA02c6ipgjKT7gK8TXdMfA75GWIc3EZGXk6vjjwI2kPRHSX8m\n",
       "WhsNGzKgJEmSZBgiMRbYmwgSGQf8D3C2zavNGb+xtVPSfLZfqSy3i4Ef2/5lM2RoJbnnliRJMoyQ\n",
       "WJCwgo4CHga+BFzZxty0SZK2BeYBrh4Jig3SckuSJBkWSCxPVBH5KPBr4H9s7m7dfJ29duaeW5Ik\n",
       "SRuR2EjiPOAuYAqwjs1+rVRscwLplkySJBliJEYDOxP7acsRTZsPsnmprYJ1EKnckiRJhgiJeYED\n",
       "gE8DzwP/DVxs82Y75epEUrklSZK0GImliFq6hwC3AAcCNzUhNy2ZBbnnliRJ0iIklpU4lSgOvwgw\n",
       "wWZXm9+lYmstqdySJEmajMRiVbuZe4CXiKTrI2wens2pSZNI5ZYkSdIkJBaQqBE1GecD1rI51uaZ\n",
       "Nos2x5HKLUmSZJBIjJM4GniEaDmzUWWpPdFm0eZYMqAkSZJkgFQlsiYSVUTuALaxube9UiWQyi1J\n",
       "kqTfVM1BPwR8BfgrsIfN7e2VKulOKrckSZIGkRCRfH0C0Rz0YJsb2itV0hup3JIkSRpAYmui/cs4\n",
       "4PPAFRnOP3xJ5ZYkSdIHEhsBJwLLA18Gzmtjhf6kQTJaMkmSpBck1pS4BLgIOB9Y3eYXqdhGBqnc\n",
       "kiRJuiGxosRZwHXAb4GVbH5kM6XNoiX9IJVbkiQJM5TKug14CHiXzbdtXmuzaMkASOWWJMkcTY9S\n",
       "WZOBVWy+mu1nRjap3JIkmSORGN+tVNa8RKms42yebbNoSRNI5ZYkyRxFVSrrM8DDTC+VdWSWyuos\n",
       "MhUgSZI5gqpU1oFEqazbga1t/txeqZJWkcotSZKORmI0USqrTpTK2s3mjvZKlbSaVG5JknQk3Upl\n",
       "nUj0VMtSWXMQqdySJOk4epTKOp4slTXHkcotSZKOoLLUJgCTgHcQe2vnZ0WROZNUbkmSjGgkFgD2\n",
       "Aw4H5gFOAs7MiiJzNqnckiQZkUisTSi0DwLXA0cD16ellkAqtyRJRhAS8wB7Ekrt7cCPgDUzRy3p\n",
       "SSq3JEmGPRIrAocBHwV+T7geL7d5s62CJcOWVG5JkgxLJMYAOxFW2nrAmcBmNo+0U65kZJDKLUmS\n",
       "YYXEMsBBwMHAP4BTgV2yOn/SH9paW1LSDpIekPSwpOP6OG5DSW9K2n0o5UuSZGiQkMQ2EhcC9wJL\n",
       "AzvZTLA5OxVb0l/aZrlJGg18D9gWeBy4Q9Jltu/v5bhvAFcBGnJBkyRpGRKLAAcAhwKvE1baxGw3\n",
       "kwyWdlpuGwGP2H7U9hTgXGCXXo77BHAh8PRQCpckSWuorLSNJc4kaj2uRxQ0Xsfm1FRsSTNop3Jb\n",
       "Fvhnt8ePVc+9haRlCYV3avVUls9JkhGKxHwSBwN3AecAfwZWstnP5uYsj5U0k3YGlDTyRT4Z+Jxt\n",
       "SxKzcEtKmtTtYWm7HLx4SZI0A4k1iIjHDwO/JWo9XpvJ1kOLpAIo2izGkNFO5fY4sFy3x8sR1lt3\n",
       "1gfODb3GYsD7JE2xfVn3g2xPaqGcSZL0E4m5gd0JpfYu4HTC7fjPPk9MWkZ10V92PZZUa5swQ0A7\n",
       "ldudwEqSlgeeIEro7NP9ANsrdN2XdAbwq56KLUmS4YPE8kRwyIFE1ON3gV9mncdhhLQQ8J12i9Fq\n",
       "2rbnZvtN4OPA1cB9wHm275d0qKRD2yVXkiT9Q2K0xE4SVxAXrXMDW9hsY3NhKrZhhLQdcA/w8swv\n",
       "aabnqufPlLRHdb+UtH5rhWwObU3itn0lcGWP506bxbETh0SoJEkaQmIp4GPAIcCTRODXnjavtlWw\n",
       "ZGak+YFvEhVfPoZ9LdIRPY6aVRyEu73mPo4bVmSFkiRJGkJiLmBTYLvqtjJwPrC7zV3tlC3pA+k9\n",
       "ROmym4C1sV/o+3AJOIXIQf4n8EarRWwFqdySJOmVqvnnykxXZlsADwLXAJ8B/s8emQvfHIE0D/AV\n",
       "4CPA4diXNnjmbsT/fTVgKWLb6MctkbGFpHJLkuQtqooh2zBdoY0ilNlZwAE2z7ZRvKRRpPWAnxEX\n",
       "I2tjz1AEQ3WN7ePsLYBzbBv4l6TrWydo60jlliRzMBJjgU2YrsxWA35HKLT/AR7I5OoRhDSWyCP8\n",
       "OPBp4BxCSaG65gW2Jyyz9/cxiumAUoeq3veIRZJtj/h/RJIMBZWr8V1MV2YF8AihzK4BbrF5vW0C\n",
       "JgNHWh34KfAsETTyuOpahAgi2Y2wyO8ALgF+yST+0X3tlPQScCwRHHQosCOwJFFJ5iDbF0v6PfCS\n",
       "7S2G8J0NiFRuSdLhSCwMbM10hTYX05XZb+ys2zqiieLynwI+B3xh3i9wxatj2ZVQaBsB1xMK7XLX\n",
       "/Oz002ZcOyvl9qjttSSdAryXaDn0BvCTVG5DTCq3JJmRqsnnRoQLajtgTSJSrkuh3Zeuxg5BWgE4\n",
       "8+WxjHv/vtzw2+XZElgJuIJQaFe75ld6P3Um5XYu8AFin+5awjW5A+GmPMH2+VUJrzrwEuEBuAE4\n",
       "oiqR+L/AhsA44MKuylGSNiRKKc5HdH7YGniN6PayPTAN+JHt70nahuiyPoawMg+3PaCgpVRuSdIB\n",
       "SKzAdMtsa+BRQpFdDdyc/dA6i0mFtO1fOXHdJ/nUNyYw+WtbMG3qKC4lFFrpmmebON+LcnsHcHll\n",
       "ue1BuCa3BxYnFM3GwKpEbvJqhFV3FXCa7YskLWz7+apN2W+AowhFeT+wt+27FPl2rxK5kVsBH7I9\n",
       "TdLC1fMPAVvbfkTST4G7bQ+omkoGlCTJCERiQWJx6FJo8xPK7GLgCJsn2yhe0gJU1xhgizX+zX7f\n",
       "/Tv7jH8d7fohzvrNivwYuN01D7YQdXcj4T1Mj5j8t6QbCatsMnC77UcBJP2iOvYi4IOSDib0ytLA\n",
       "6tVY/7J9F4Dtl6vztgFOtUPmSimuA/zN9iPVeT8FjmSApcJSuSXJCKByNW7AdGW2DnArodB2B/6U\n",
       "rsbOQ3WNI/7fu8nsdOidTD7pGpb41wKcPmUUR1/7s9lbaL1RqnzXbA7pLWLSPf5SHeOqRvBngA1s\n",
       "v1jVAp6HvquZzGr8Wb3eL1K5JckwRaJrYduDiHh7jFBmXwF+l2WuOhPVtRDTIxy3Be5e4ymuueXH\n",
       "LDH+Dd4ObLnSsx5wRZhS5ZbAeb289BKwQHX/JuCQyjW4KJH79lnCGtuoUmb/APYGTgPGA68AkyUt\n",
       "CbyP2I97EFha0ga275S0APAfYk/vUEk32J5auSUfApaXtKLtvxDJ5+VA32cqtyQZRkjMT4Rg70Hs\n",
       "d/yBcPl83p6pJVTSIaiuZYjGzLsReYclsX92qCcxAfgBkUi/O/aA909LlROJQI4PEwqmO28SLsg/\n",
       "Eftq9wB/JCyqY2z/W9JqxP7b94iAkuttXwJQRVI+QJTsugnA9hRJHwROkTSOUGzbEi2QVgbukTQF\n",
       "+KHt/5U0EbhA0hjg9up9D4gMKEmSNiOxELAzodC2JtyNFwGX2vy7nbIlrUN1rUQos92IQI1fEwrt\n",
       "Ktf8crfWNBOAA7BvGuhcpcpRwNepvACFiwd6CShZnmgrttZA5xlOpOWWJG1AYnHiSn0PYvEqCYV2\n",
       "oM1zbRQtaRGqS8C6TFdoiwKXAjUiwnF6yLv0XqKe46+Ad1MFYgyEUuV8wNnAIsDGhYtZlVD7L2DF\n",
       "ygJ7GPi57V+GOPo54cpcpJJ9PLAscLbtr1TH7Ad8gsijvI1IEZg2qxSBVpOWW5IMERLLEAvDHkSX\n",
       "+WsIhXaFzUvtlC1pDZVC24RoxrwrMIWwzi4BbpspwlGaj2hNszNwEPY1g5m/VPk24DLCvXho4eIt\n",
       "BTqbVIAtgE/b3k3SgsDvCTfk/sDXgDWI0P07gAMId+M3gN2qPbT/BW61fVZvKQK2/zSY99UIabkl\n",
       "SQupOlPvTii01Ynk2lOAq23+00bRkhaiupYjAiIOIJKUzyGCRP7s2iwsCmkCEf5+Cw20ppkdpcoN\n",
       "CMvwu8BJhYvZWTJvKTrbv5X0v5IWA/YkLK5p0Q2Ha2w/HyLrYiIVYCpxwXZndcw4eCsdpbcUgVRu\n",
       "STLSkFiZUGZ7AMsDvwROBK7Luo2dS1WYeFdCoW1A9Lrbn7DQZq1YojVNvTr2CKoAjcFQqtyDCMY4\n",
       "pHAx0PF+RijoDxLvqTfE9BD+n9r+/AwvSu+k9xSBlpPKLUkGSVWMeE2mK7TFCLfTccCNNm+2Ubyk\n",
       "hVRux82IxX8PYq/px8Aurnn2qRrSukQU5EPAOtiDCiAqVYqoMXk4sF3h4vf9OL17KgBEg9M7gCds\n",
       "P9Dt+fdWofuvEfvGEwkX5S8lfdv205IWIQoLLEDvKQItJ5VbkgyASqFtwHSX49xEdZDDicr6g60W\n",
       "kQxjVNfbme52fJNQBGu55scbG0BjCSX0CeBo4OcMMgCiVDk38ENiP2yTwsUT/Tnf9rOSbq5SAX5t\n",
       "+zhJ9xEXam8dRoToXwS8DTjL9t3xlvRF4BpJo4i9xSNs395bisBQkAElSdIgEqOIq/Q9CKX2OvEj\n",
       "vwi4KyuEdDaV23F3QqGtS0QPngnc0afbcaaBtBrh8nuWCBoZdP5iqXJx4uLqKWD/wsVs93Nnt3ZK\n",
       "mpfIdVvX9kvVcwcA69v+xGBlbjVpuSVJH1Rlr7YkFNpuwDOEMtsJuDcVWmdTuR0jzyy+A7cQ1tFl\n",
       "rvUzmTosmk8Bnwe+CJw2WGsNoFS5OpEycC7wpcLFoL0GkroSrf+nS7FVmL5Lag0b0nJLkh5IzE00\n",
       "dtyDaAHydyoLzeahdsqWDA2q6x1EgMdHCQv9TODnrrlfrr7pA2oF4AxgFJGQ/ZdmyFmq3J7Ys/ts\n",
       "4eJn/ROps9fOVG5JUlHVcvws8GmiTcdFwMU2j7ZTrmRoUF3zMd3tuA5hCZ0J3NUvt+MMg2oBYh/2\n",
       "GCJJ+mTsqU0Ql1LlkYQFuFfhot97WZ2+dqZbMpnjqYJDdgf+G7gL2MDmr+2VKhkKKrfj5oRC2w24\n",
       "GTgV+JVrHnjahrQo0c/sCCJxeXNmjDgcMKXKMcC3Ce/ChMJFfld7IZVbMkcjsRZRv29x4GM217dZ\n",
       "pGQIUF3LM93t+CrhMvy8ax5cHzxpGSL68UDC8t8M++FBjdmNUuWCRCCLgE0LFy82a+xOI5VbMkci\n",
       "sQiROPtBooXMDzIfrbNRXfMT+6gHEHmJ5xItW+4esNvxrcH1TuBY4vv0MyJn7Z+DGrMHpcp3ApcT\n",
       "eWKfKlzk97UPUrklcxQSo4kW95OIK+vVbGZVSDYZ4aiuUUQvsgOIhOPfEeXPrhiU2/GtCbQ6cDzR\n",
       "pugHwKqDTcTujVLlBOBC4MTCxfeaPX4n0i/lVhW+nM/25BbJkyQtQ2JLos7e88B2Nn9ss0hJi1Bd\n",
       "KzDd7fgS4XY8zjU/1ZwJtAER0j+BcGt/HLslLsJS5X7A/xD5a1e1Yo5OZLbKTdIvgEOJwph3AAtK\n",
       "+o7tb7ZauCRpBhJvB04iqrMfA1yQ+Wmdh+oaS7gFDyKK8/6CCBT6w6DdjgB6K/jkC9X4JwH7Ybek\n",
       "AHbVg+0rRGPRrQoXf27FPJ1KI5bb6rYnS9qX6M76OeBuoi1DkgxbJOYllNlRhCtqYlbi7zxU13jg\n",
       "YCJB+kHCkrpihv5og5pAImoifh5YkgjpPws3afxeKFXOS3QIWJrowfZ0q+bqVBpRbmMUddB2Bb5f\n",
       "tQ3Pq95k2FKF9u9JXFnfDqxn8/f2SpU0G9W1DHHhchBwLVGs+O7mTaDRhOX3eWA00cfsQuyWBnKU\n",
       "KpcmerDdD2xTuMhOEgOgEeV2GvAoUWPst1Ur8gw/TYYlEmsTV+6LAAfYlO2VKGk2qmt1Itl+V6LD\n",
       "9Iau+W/Nm0BjgX0JL9ULwJeBy5tRKmt2lCrfTSi2HwBfb6AHWzIL+l2hRGGij3aLr14apdOz7JPG\n",
       "kFiU2J/YC6gBP8rQ/s6hW7L1McCGwPeAU11z8yJdpXFEftqxwMOEpXbDUCg1gFLlLsCPgCMKFxe2\n",
       "er5OXzsbCShZimi0uKztHYDVgE2JnkVJ0laqwsaHEgrtfGBVm+faK1XSLFTXaMJCOxZYmKgis3dD\n",
       "vdIankTjiRJZnyLc2Htj39a08WdD1YPts8AngR0LF3cO1dydTCNuyTOJMNovVI8fJhaRVG5JW5HY\n",
       "inBBPgtsa3NPm0VKmoTqGkfkph0NPE0EcVzmWnPqMsYkWpRQKEcAVwPbYf+paeM3QKlyLqLc13pE\n",
       "D7ZBt79JgkaU22K2z5P0OYAqoCTdPUnbkHgHcQW/IXHFe1GG9ncGqmsxQtkcCfwfcIBrvrm5k2gZ\n",
       "4DNEB+kLgU2wH2nqHA1QqlyUKCTwArB54eLloZahk2lEub2suMIBQNImZEBJ0gaq0P7jgI8TFtv+\n",
       "Ns1zTyVto0q6PprI6boYKFzz/c2dRCsQ7s29iTD7tZvRKHQglCpXIUppXQwc34webMmMNKLcPkM0\n",
       "wltB0i1Egdk9mzG5pB2Ak4kw29Ntf6PH6/sSX0YRVQYOt52upzmMKrR/byK0/xZgXZt/tFeqpBmo\n",
       "rg2IIJFtiCaga7jmfzV3Eq1BRD6+j4hCXAW7bXljpcptgHMIpfaTdsnR6cw2WlLSPER1klUIJfMg\n",
       "MMruZxfamccdXY21LfA4Uf1kH3v61ZqkTYH7bL9YKcJJtjfpMU5HR/zM6Ui8m7DSFgQ+aXNjm0VK\n",
       "BkkV+fg+QqmtSLRvOd21GTo+N2EibUjkqG1KXESf2qoSWY1SqjyEiOr9UOGibKcsnb52NmK53WJ7\n",
       "PeDerick3U1sgA6GjYBHbD9ajXkuUdj0LeVm+9Zux98GvG2QcyYjBInFgK8SSbRdof3NCyZIhhzV\n",
       "NRewD7FPOpWwxM93zVOaN4kEbEkotVWrOfZtVYmsRilVjq5k2RF4T+FiyPf45jRmqdwkLQ0sA8wr\n",
       "aT3CajMwHpi3CXMvC3RvCfEYsHEfx38M+HUT5k2GMVVo/+HAl4iWJKtlaP/IpiqPdQgRan8/sbf2\n",
       "m6bUe3xrEo0C3k+4HxcnoivPbmWJrEYpVS5AuCHHET3Ynm+zSHMEfVlu2xGhuMsC3+r2/EvEVdFg\n",
       "afiLLWkrIrlywixen9TtYWm7HJRkSVuQ2IZwQT4FbG1P9xYkIw/VtSwRav8x4CpgZ9f8++ZOojFE\n",
       "seTPAW8CXwcuwk1MGRgEpcqNiSoq1wKfLFw0z0rtJ5IKoGjX/ENNI3tue9puerZ8FXU5qUoMR9Lx\n",
       "wLRegkrWJiKKdnAv4bqd7jeeE5B4JxHavx4RwHRJhvaPXFTXGoTrcReicefJrsX2Q/Mm0TgilP8Y\n",
       "4O+EpXb1UFUTmR2lyjFEn7cjiYojF7dZpJno9LWzkT23y6uoxeWJqEYBtv2VQc59J7BSVavyCeLq\n",
       "a5/uB0h6O6HY9utNsSUjG4n5iCvuI4iggv0ytH9kUgWJbEkom/WI8ljvcs3NdSlLCxJu608SQWgf\n",
       "Zsa9+bZTdcw+G/gPsH7h4vE2izRH0ohy+yWRZHgXMKgIye7YflPSx4nKAKOBH9u+X9Kh1eunEQVL\n",
       "FwZOjX1iptjeqFkyJO2hCu3fg3B33wK8255h/zUZIVTlsXYnlNqChAW+h2uDi6aeeSItSezZHULs\n",
       "vQ95NZHZUZXR+gjxGXwd+E7mr7WPRtyS99pec4jk6Tedblp3GhKrEd2wlwY+nlX7RyZV5OPHCPfj\n",
       "k0Qk4GWuubmLufTOao59iOaj/42b2AGgSZQqFyZy6FYH9i1cDPt83E5fOxtKBZC0diZPJ4NBYgEi\n",
       "AvJA4ATB9F3nAAAgAElEQVTg+zZt21xPBkZlqe0HTAIeAPZvenksAGlNpide/xBYDfupps/TBEqV\n",
       "WxM1eC8GPlq4aK7VmgyIRpTb5sBESX8Duprm2fbarRMr6RQqF+SHiCv764A1bZ5sr1RJf6n21HYn\n",
       "cg+fJZTa75o/kTYlAjE2JCJnj2x34vWsKFXOTVyo7QN8rHBxdZtFSrrRiHJ7X8ulSDoSiTWBU4h9\n",
       "0w/aNP8KP2kplVLbjmh7NYrIUbu6yTlqArYnlNrbiQuhD+ImtrVpMqXK1YGfE42c3124eKa9EiU9\n",
       "ma1ys/2opM2Bd9k+Q9LiwPytFy0ZqUgsSFQV2Q+oA6dl49CRh+p6D6HUliBcyhc3dU8tSvDtSbgf\n",
       "xxDh/OcxTBoh90YVNHIk8f0+HvhxdssenjTSrHQSsD5RW/IMYC4izLXXhOpkzqVyQe4HfIOIaFvT\n",
       "5t/tlSrpL6prXcLdtgaxt3a2a01UONLcwP5EUfSniajoK3CTg1GaTKlyKeAnwGLAZoWLh9ssUtIH\n",
       "jbgldwPWJVIBsP24pAVaKlUy4pBYB/g+MA+wm82QdTJOmoPqWoUo6rsFYbHt7ppf7/us/kygBYhQ\n",
       "/qOBe4hoy98Nl8TrvihV7kwEtpwOfKWdlUaSxmhEub1ue1qVZ4ak+VorUjKSkFiYWBD3Jq7AT88C\n",
       "xyML1fUO4n/3ASL38EDX/ErzJtBiwFFE8vX1wE64yWW4WkSpcj7iM9ke2KtwcVObRUoapBHldoGk\n",
       "04CFJB1ChHKf3lqxkuGOxCii9ujXgEuB1W2ebatQSb9QXUsSdWL3A04FVnLNLzRvAr2dKKf2EaLj\n",
       "9WbYI8aVV6pcnyh4fBsRNDIsozaT3mkkoOQkSdsRBZNXBr5k+9qWS5YMWyTWJ1yQADvZ3NlOeZL+\n",
       "oboWJhKjDwPOAlZ3rYk5ZNKqRMf0XYg9qrWwR0wJqqo9zbHAp4FPFC7Oa7NIyQBoxHLD9jXANS2W\n",
       "JRnmSCxK7MXsSlzxn2kzrIMAkumorvkI9+DRRFm9dV1z8zqaR3PQzxG5sacA78JNri3ZYkqV7yAU\n",
       "/lSiLmSWhRuh9NXP7WbbEyS9zMztaWx7fGtFS4YLEqOBg4i9tfOJHmvZk2qEoLrmJgI5Pg/cCLzH\n",
       "NT/YnMElYGsiLH4Voq7i/riJe3ZDRKlyX6KA90nAt7Iu5MhmlsrN9oTqb+a0zcFIbEy4IF8Dtrf5\n",
       "Q5tFShpEdY0h9rtqwL3A+1xzc/5/0Rx0F0KpLUCkf5wzHJqD9pdS5ULA/wLvBrYvXIyIYJekbxrJ\n",
       "c9sEuM/25OrxeGA12xnq3cFIb3Uzfh+xf3J29lgbGaiuUUTXha8SjV/3bVr9x1BquxPRlVMIN/Wl\n",
       "wz1HbVaUKrcges5dDmxQuPhPm0VKmkQje24/IPozdfFK9dy6LZEoaSuVC/Iw4mr/54QLMqPERgBV\n",
       "qawdCIUzjdhfu7YppbJCqe1JVCp5jXBxXjESctR6o1Q5F1E956PAwYWLK9os0pAhMZZo9NrRNBpQ\n",
       "Mq3b/amKsjlJhyExgWgy+SKwtc29bRYpaRDVtTmRlrEo8EXgkiYptdHAXoRSe5mw4q8cqUoNoFS5\n",
       "KlFl6UkixH+OqKJTpe/sSVSfaShQRtKjwHoeYYFB0Jhy+5uko4g8GBGJmH9tqVTJkCKxFLFnsg0R\n",
       "In5euiBHBqprPcJSW5XK2nbNg0+iD6X2IUJRvkDkq109wpWagEMJd+2XgNPmlLqQEtsS2wwCjrS5\n",
       "VmroN+7qnBFHI8rtMKK55Berx9cRkVfJCEdiDPBx4AtE3dDVbF5qr1RJI6iu1Yjo1QnElfgurjUh\n",
       "mEMaQ7Rw+SLwDPBJ4NqRrNQASpVLEMUnlgU2L1w80GaRhgSJDYiu4MsTv/MLZ5W+U1WfOp/4jEYT\n",
       "FwEAx0p6H/Aq8GHbf5F0JuGeXh8YDxxte1i5dhtJ4n4K+OAQyJIMIRJbEi7IJ4HNbeaIH/tIR3Ut\n",
       "T1ho7yfC7j/qmgcfBBFKbV9iAXyKqHx/3UhXagClyh0JxfZTYM/CxYiL6OwvEisTFz0TiIugnzTQ\n",
       "HHgH4HHb748xNJ7w6Lxge21JHwFOBnaujn+77Q0lvQu4QdKKHkbRsn3luR1n+xuSTunlZds+qoVy\n",
       "JS1CYhliUXwPkcx7Uboghz+qaylC8XyYSM1YuSmlsqSxRPmtLwCPEW67skOU2rzAN4nFeJ/CxY1t\n",
       "FqnlVL/vGhEt+y3gAJtGL37uAf5b0n8Bl9u+qaop/Ivq9XOJPEAId+X5ALYfkfRXYDXgj015I02g\n",
       "L8vtvurvXcyYxC1mTupOhjkScxEupuOA04CDbUZcou2chupaAjiGqKD/U2A11zz4AIhQavsTUY//\n",
       "AA7CLgc97jChVLkuEe37B2CdwkXzamYOQ6oC5scBBwM/Bla26VcQiO2HJa1LeAVOkHR9b4f1McSw\n",
       "SgfpS7ntDfwKWMj2yUMkT9Jkqh5rOxNXsH8DNrUZMcVr51RU12KEUjuYuHJe2zU/NviBNRcR/v55\n",
       "IjBsIvZvBz3uMKFUOYoIijoG+FTh4udtFqmlSIwDPkG830uBdWwG9D2RtDTwvO2fS3qRuKCC2Jb6\n",
       "RvX3lq7Dgb0k/RRYobo1p+pNk+hLua0vaRngQEk/6/niSAwNndOQWI9wTSxBFIG9Kl2QwxvVtSix\n",
       "OB9CuH3Wcc2Dr28YSm0iUVHkIeAj2B3VvqVKyP4voi7khoWLR9srUeuogsEmEi7I2+jHvrnKcg0i\n",
       "wrYnawEnSZoGvAEcQXRzWFjSH4kAkn2qY01Y/LcTASWHDqf9NgDNyrVehf8fTmjkJ3q+bvudrRWt\n",
       "MSTZ9ogMVW0VEssRX973Ep2Uf2zTvE7KSdNRXYsQ4faHEQvK11zz3wc/sOYm2lQdT2w11LFvHfS4\n",
       "w4jKBfk1Ih3iy8A5hYuO7ClYeWL2IH7fjwOfs7m9oXPL8h1E4vqOwH+x1Vbf6u/aKekAIkJyfuB1\n",
       "4GbbZ/VnjD7GXgb4ju29JK0DLGP7yoGO11dtye8C35X0A9uHDXSCZOiQWIDwux9OVJFZxWZye6VK\n",
       "+qJqP3M0cZV8MbC+a3508ANrHsKt9DngT8De2P836HGHEaXKlYhw9S2JxX6XTo6ElNiasEzHEK7I\n",
       "axvxxKgslyDc0B8hgpFWclG8qPDqDIZrbF88yDHewvYTRMEAiApY6wPNV27dJjxM0ubAu2yfIWlx\n",
       "YH7bfxvopElzqVwUBxJW2m+Ad9uNVSBI2oPqWohwFR9JtJ/Z0DUPvjiCNI7YpzuWCKbYA7uhK/uR\n",
       "QqlyWcJC252I3vtY4aJjg6Oq/olfJ7xoXwTOb6TVlMpyPHHh9AkiuGZ1F0WfffskXQIsB8xDWFE/\n",
       "kjSRuEh6gYiGfN32REmTJL3T9rcklcT3bUtCrxxo+w5JixA9/d4J/Ac4xPafJG1JpBVAuDg3BxYn\n",
       "4jzWI9IX5pE0gVDoJwCb2X5GUQruQWAT27NskNxI4eRJwAZEo9IzgLmqD2qz2Z2btB6JHYjQ/meA\n",
       "nW3uarNISR+orgWJqNVPEMV6N3bNfxn8wBpHhPEfC9wJ7IrdUU1kS5WLEovsgUTe2iqFi47d+5dY\n",
       "iVjUNycs1NMbyFVDZTk34d4+nujDuYGLolFj5EDbzyu+T7dLuoK4aF4PmAzcANxdHWumR08aGGd7\n",
       "3coYiia14Qa9y/aukrYiilSvS7jgj7B9q6R5CRdnDGRPkfQlYP2ulDNFA9x9ge8A2wJ/6EuxQWMV\n",
       "SnarhLmrmvhxSdkGp81IrEUoteWJBe2yDBYZvqiu8UQh408CvwY2dc2PDH5gzUssZMcQgQU7Yd/d\n",
       "90kji1Ll/MCnqtsFwFqFi5niADoFiaUJy3Qv4H+AAxtJ21FZjiZyFuuEK3o7F8U9/Zz+k5J2re4v\n",
       "R7gyb+hSJJLOIwyd3vgFgO3fSRovaUEiiXz36vkbJC0qaQHgZuDbkn4OXFzplRneDjOW/foJ4eH4\n",
       "DnFxc8bs3kgjyu1129O6Jq5KtCRtoqoD+VXgA8RV3Q8auZpL2oPqWoCw0j4FXA1McM0PDX5gzUfs\n",
       "rX4GuBV4H25Sr7ZhQqlybsIaPZ6wGDYpXAz+gmCYIrEQcaF6KLF4r2LTp3UCoLIUsR58DXge2M9F\n",
       "0e9IWEkFUV92E9uvSboBeABYvfth/Riy62K75zmuCoRcTuTU3Sxpe7pZbzMNZD8m6SlJWwMbMj1q\n",
       "c5Y0otwukHQasJCkQ5juEkiGEIl5iYXsk0z/4nd0YupIRnXNT9TtPBq4FtjCNQ++xFl4TY6oxv0d\n",
       "sD12f6/OhzWlZrBA7gV2KFwMm8oXzabKVTuSUGy/Ata1+UdD55ZvpT/MX53/axcDLgY9nshze61y\n",
       "A24CjAO2rPbOXiKsya5mrt2tKxF5cKWk9xAluyZL+h3hTjyhUp5P2365KtX1Z+DPkjYkurh3/x5P\n",
       "Jprgdud0opvDT91ABZ1GAkpOkrRd9cZWBr5k+9rZnZc0h6pNxUeIaLCbgA1tMphnmKK65iMWqs8A\n",
       "1wNbuub7Bz+wFqjG/TRQAttid1RLoqpq/66ER+I5YL/C/bdARgpVINhHiT2tO4HCfqsyVN/nluW7\n",
       "CUttNaLDwS9cDDr94SrgMEn3EQEbtxJpYJOq+y8wXbHBzHtur0m6G94KcKM69ydVntwrxPuFcH9u\n",
       "RVQ1uZeIily223g3AJ+T9Hvg67bPJxT/GTTgkoQ+8txmOEhaijAFAW6zm1D+p0l0cp5bFfr7LaIa\n",
       "99E2HRXK3UlUSu1wIgH7RuArrvnPgxtUiwHbE93QdyAswBPwIMcdhpQqtyYiAucmwtav7NR2NFWu\n",
       "2m7EBetTRK5aQ79tleW7iEjCrarzf+hiYOkPzVw7KxfmZ9zC/V5JGwDfsr1lI8c3Ei25N3AS8YMF\n",
       "OEXSMbYvGLiYSV9IrEp85msQeWsXZrDI8ET1GQI6bgK2dW2AFlWEOK9PJNm+j7gqv4G4qj0eN6FS\n",
       "yTCjVLkhYYG8k7BAzitcDKsahc1EYivCjTg3YYVf3WCu2tLE57M3EUJ/iIvi5VbKOpyQ9Dnid/bh\n",
       "hs+ZneUm6R5g2y5rrcpzu8722oOQtWl0kuUmsThhxu9N/AC+Z896kzVpH6rPEHp/K1B3bQB7X9Ki\n",
       "wHZMt86eIaIprwRuwu7I/3/VDfsEYFOqliyFi44NjJJYl7BMVyKU1LkN5qotRFw4HQacCXzdRfFM\n",
       "c2TqnLWzNxoJKBHwdLfHzzJCO7MOVyTmIQJFjiFyCFdtJEoqGXpU1zxE3cfjiLp6O7rWjyjFsM7W\n",
       "I5TZ+4A1iT20XwNfxk2oTjKMKVW+nbiA24lIZdm/cDH4fnTDiGqffBViK2eD6u/yhDL/kc1s3Ygq\n",
       "y3FEQNIxdAWZFEVDQSZJ0Ihyuwq4WtI5TI+IGXBJlGQ6le/9Q8QV3d3AZjaDDxNPmk6l1A4ikojv\n",
       "AnZyzb/v+6yuk7UIM1pnzxG/oRrw2061zrpTqlyc2EvbnygNt3IntKGpfsPvZEZFth5hENxBBIpc\n",
       "BNzRSF81leUY4ADiu3E7sKWLYvABSXMgfTUrXQlY0vYxkvYgkvEgWh6cMxTCdTISE4gEzVHA/jYd\n",
       "03akk1BdcxM1Go8nygvt4pr7rgIT1tm7mb53thaxZ30lMIk5qHRdqXI8ETn6cWLdWKNw8WR7pRo4\n",
       "EssyoyLbgAj46lJkXwPu7G8vtSpXrXtB5D1dFLc1UfQ5jr66AlwBHO8eOTSS1gZOtL1zrycOMSPN\n",
       "byyxItEbaUPiSvYXjfjek6FFdc1FhDN/nghVnuRaHzUapYWJLgxd7sYXmb539lvs11ot83CiVDkP\n",
       "kY93HJG8XivccAmoYYHEYsysyMYyXZHdQSiyfw1qnrLclvDejCY8A9cOIlet8XlH2NrZX/pySy7Z\n",
       "U7EB2L5HUlPa3UjagYj8GQ2cbvsbvRzzXWKx+A9wgN2gK2iYIbEIUfR0fyK8/yM2r7ZXqqQL1TUa\n",
       "WIQo3ro5odTuB/Z2rZdq+lGyZx2mW2frEEnVVxLh+oOvFzkCKfWWW+3LhKt9m8LFsM/Hk1iQiFTt\n",
       "UmQbAgsRLug7iZqInwD+0azIZZXlBoRSWx74AnChi9ZHiio6RizW6nnaTV/KbaE+XptnsBNLGg18\n",
       "jyiC+Thwh6TL7OkJr5J2JLoRrCRpY+BUImt+xCAxF3EF+3miT9fqNsMmT7BTqUL0Fyd+xH397bq/\n",
       "EJGk+jSRwPoh13r0PYtaee8lFNoOwMuEMjsRuBF7jr1YqTpg70GUhvsXsHfhYljmZVbVftZlRkW2\n",
       "LOF2vpPoaP0F4JFWeFVUlqtQVbmnihR1MbBIUUljiIuy7t/rnreez48lonI7mr6U252SDrH9w+5P\n",
       "SjoYmlJ5fiPgEVfRYZLOBXYhrpa7+ADwUwDbt0laSNKStvts2zAcqDaadydckA/Qj+oDyYyorlHA\n",
       "wsyojGb3dzShqJ4mfsjd79/V7bmuv8+55hkrPIR1tjZhme1ILIhd1tnXcBMKH49wqqoi2xF7TSaK\n",
       "Q187XBKwq4vLtZlRkb0L+DPhVrye+I3e3+qGvirLtxGBIrsSkaIfdTE9UlTxfVuQvhVTz+fHE/Uk\n",
       "n2HG7/QzwD+JiiI9n3/ZtiU19D+S9Ciwnu3+7SNKZwK/sn1Rg8cvXx2/Vn/mmeV4fey5LQVcQrQb\n",
       "71Jm6xPJh7vZHpyfWdoT2N72wdXj/YCNbX+i2zG/Ikqv3FI9/g1wnD19Q1+S19t+72G1Qf3m1HGj\n",
       "Hn182wWnTptbK73jl88vttB9c9R+S7+QZU0bbU0bY3kMo6aNmsa0MZJHT8OjPWraaLCMGOVRU2W9\n",
       "KWuqGDWVaaOmilFTNU1vyqOmxk1T5dFvahCuo7HTpo1daMqr48e/+fqC09C0l8bOPfn5uca9+MLY\n",
       "eV6ahnJ/tGLs1NFj1v37Mm9b4sUF57pv2cn3PLj0S4+1XaNZvPn6ggu/8Z9FlnzztQWXmjpl3KKj\n",
       "x7z2wuh5Jj8517jnn5p7/qeenHuBJ5/R6ClD2Klb+sNcG6740tjFtuG++27luuv+yCuvzM/MCmtR\n",
       "IjjlGXpXVr09/4LtAb2XRvfcJP2NaD/TX+V2BnB5u5RbX524n5S0GVHmZU3iquxy29c3Y2JoePGZ\n",
       "qaJ0zwPu/ne51FsHLz0/WrrdHXnMmBWuYvSYp3kQlnywzdJ0EKOJfoJDzTzAEm2Yd3gjuPmdZtpc\n",
       "ZuzLes/Yl0Yx9iUxdvIoxr4s5nppFKOmtCNeoeuK3QY8FRaZCou8Aau3o6SHNI3lxtw37cG/jL5q\n",
       "6suv/Z2o0/s3elFatlvWSbwqXFzM5piZmpX2eH1/IvrVwD2296+U0k8I5fw0MNHTq+lsIeloYCng\n",
       "WNsXVRbqNwnXvoETqtqRTaXPPLeq8vL11a3ZPE58iF0sBzw2m2PeVj03A777qY6N+EmS4Y7qGv/G\n",
       "Ql7pjYWmrkIkL69CFFlfmdiXfBB4qPrbdfuba+7YiiTdKUsJOBumvFYU0z1TQ43tkigYAICkWi+H\n",
       "9WxWelG349cg9iI3tf2cpK64jFOAM2yfVXXt/i5RO1PAUrYnSFoNuIzI+dudCMBam3Cv3iGpq7xj\n",
       "02iocHIrqDZCHyT6Bz1BJCzu00tAycdt7yhpE+Bk25v0GKejw1mTZKSiukQEanQpu+7Kbxng78yo\n",
       "8LoU4L9da9PC1CLKUvMTwSpfLwr/tN3yQO9rp6RJxJ4gwDsI6+pcYs/yw8AStr/U45ynCSU2VdJY\n",
       "4Anbi1duyWts/6I6brLt8ZK+DfzR9pnV8z8jmtD+iaFwS7Ya229K+jiRAzMa+LHt+yUdWr1+mu1f\n",
       "S9pR0iNEu4SJ7ZI3SZL+USmox6rbdd1fqyq+rMh0ZTeByCtcBRitunqz9h5xzSOyVFdR+OWy1F7A\n",
       "9WWpO4ti+HV2mEWz0u6R8WbWpRdn9fwbvRzT2zhNv5hpm+XWLNJyS5LOQnUtxoyWXtf9FYB/07u1\n",
       "90/XPOyDfcpSE4l6kRsVhdta1b/n2inpA8BBtj9QuRHvJiy3M4lgwq4gwy635MKVC/OXwAW2z5Z0\n",
       "ALCz7T16BpRIesn2ApJ2I4qO70js091BRM/Py1BES44UUrklyZxBlWi/PDO7OFcmUkUeIZTdya75\n",
       "5jaJOVvKUmcS3qr9i6J9C3Avym0uIsdveeKCYUGiG/oZwAaVQtufUM5TgbttHyjp7dUxixEXHxNt\n",
       "P1Ypt1/Zvrgaf7Lt8dX9bxIpNga+avuCKjDlsmZ1nEnlliTJiEd1zU8ouQ2IBOlDXfMl7ZWqd8pS\n",
       "8wG3AScXhU9vlxydvnamckuSpKNQXesRbWJOcM2ntlue3ihLrQb8FtimKAbQB7AJdPramcotSZKO\n",
       "Q3WtQLTrOg/48nCMvixL7UfU4NygKDx5qOfv9LUzlVuSJB2J6locuIIIMT/UNbe0vNZAKEv9kCih\n",
       "tc9Q7791+to5qt0CJEmStALX/DSwNbA0cKnqmq/NIvXGJ4FVgcPaLUinkcotSZKOxTW/TBRkfxq4\n",
       "vkozGDYUhV8F9ga+UpZar93ydBKp3JIk6WiqMl8HAr8Bbla9Of0om0VR+CGiU/kFZakF2y1Pp5B7\n",
       "bkmSzDGoriOJ3oo7uTa8Gh+Xpb5PJErvORT7b52+dqblliTJHINr/j7Rc+5q1bVNu+XpwdFEAnXb\n",
       "iit3Emm5JUkyx6G6tiCK9X7KtSjsOxwoS60I3ArsVBS+vZVzdframZZbkiRzHK75t0SR4G+orqPb\n",
       "LU8XReG/EJGT55WlFmm3PCOZtNySJJljUV3LEcneVwHHDJfiy2Wpk4F3Aru2av+t09fOtNySJJlj\n",
       "cc3/BDYnqtKfrbra0em9N44lgkuGjVU50kjlliTJHI1rfg7YjuhddoXqGt9mkSgKvwF8EDi2LLVZ\n",
       "u+UZiaRyS5Jkjsc1vwrsBTwM3Ki6lmqzSBSFHwUOAs4ty+GVfD4SSOWWJEkCuOapwJHAhcAtqmvl\n",
       "NotEUfhXRPHnn5Wlcr3uB/lhJUmSVLhmu+YTiZ5wN6qujdstE5F0viBwXLsFGUlktGSSJEkvqK73\n",
       "Ex2mJ7rmK9opS1nqbcCdwN5F4d82Y8xOXzvTckuSJOmFSqHtDJyuuia2U5ai8GPAROCcstQS7ZRl\n",
       "pJCWW5IkSR9Ue29XAT8BTmxn49Oy1IlE2sIOReGpgxmr09fOtNySJEn6wDU/BEwA9gC+r7pGt1Gc\n",
       "GjAW+EIbZRgRpOWWJEnSAFX+28XAi8B+VfrAkFOWWga4C9i3KHz9QMfp9LUzLbckSZIGcM2TgR2B\n",
       "14FrVNfC7ZCjKPwE8BHg7LLU0u2QYSSQyi1JkqRBXPMbwH7AbcBNVW3KIaco/Bvgh0SAyZh2yDDc\n",
       "SeWWJEnSD1zzNNf8WSLA5GbVtWabRPkqMJXYh0t6kHtuSZIkA0R1fRj4NrBX1UZnSClLLQncDRxY\n",
       "FL66P+d2+tqZlluSJMkAcc3nAB8GLlRduw/1/EXhp4B9gZ9Wid5JRSq3JEmSQeCarwO2B05RXUcO\n",
       "9fxF4RI4BfhF7r9NJ5VbkiTJIHHNvwfeAxyluk5UXUPt7vs68ApREzMh99ySJEmahupaDLgcuA84\n",
       "1DVPGaq5y1KLE/tvhxXF7GthdvramcotSZKkiaiu+YDzARGBJq8M1dxlqQlEovmGReF/9HVsp6+d\n",
       "6ZZMkiRpIpUy2wX4F3C96lp8qOYuCt8MfAs4ryw111DNOxxJ5ZYkSdJkXPObRBfta4hcuBWGcPr/\n",
       "Bp4h9uHmWNItmSRJ0kJU1+HAF4GdXfPdQzFnWWoRYv/tU0XhS3uVq8PXzlRuSZIkLUZ17QacBuzr\n",
       "mq8dijnLUpsAlwEbF4X/NpNMHb52plsySZKkxbjmS4DdgbNV135DMWdR+P8I1+T5Zam5h2LO4URb\n",
       "lJukRSRdK+khSddIWqiXY5aTdIOkP0u6V9JR7ZA1SZKkGbjmm4CtgBNV1zFDlAt3MvAYsQ83R9Eu\n",
       "y+1zwLW2Vwauqx73ZArwadtrAJsAR0pabQhlTJIkaSqu+T6i8en+wLdVV0vX4KKwgYnA+8tSe7Vy\n",
       "ruFGW/bcJD0AbGn7KUlLAaXtVWdzzqXAKbav6/F8R/uNkyTpPFTXQsClwFPA/q759VbOV5baALgS\n",
       "2LQo/Ah0/trZLsttSdtPVfefApbs62BJywPrEj2UkiRJRjSu+QVgByLR+yrVtWAr5ysK3wlMAi4o\n",
       "S83TyrmGCy2z3CRdCyzVy0tfAH5qe+Fuxz5ne5FZjDM/UAIn2DOHtEoyUO/2VGm7HIToSZIkQ0Ll\n",
       "lvw2sRf3Ptf8eKvmGj1axY478v0pU3j16qu5HKh1suXWTrdkYftJSUsDN/TmlpQ0lqjTdqXtk2cx\n",
       "Vkeb1kmSdDZVYMkxwBGEgru/VXOVpcYDdwFf3morzunktbNdbsnLgI9W9z9K+J5nQJKAHwP3zUqx\n",
       "JUmSjHRcs13zN4EvATeorgmtmqsoPBnYC/huq+YYLrRLuf0X8F5JDwFbV4+RtIykrmrWE4D9gK0k\n",
       "/b667dAecZMkSVqLaz6LiKK8RHXt2qp5isJ/ILaHOpqsUJIkSTKMUF3rA78CvuKaf9CKOcpS2mor\n",
       "ps1u7ayC+X5le63BzCdpZ2B129+QtCvwoN069yukckuSJBl2qK4VgauAc4Evu9b8hbqRtbNZyq3H\n",
       "mGdWY17UrDF7I8tvJUmSDDNc81+IrZntgdNV15ihmFfS0ZL+VN0+CRgYI+lsSfdJukDSuOrY9SWV\n",
       "ku6UdFWVs4yko6rKUn+UdE713AGSTpG0KbAzcJKkuyWtIOmubvOv1P3xYEjlliRJMgxxzf8mYhKW\n",
       "Bi6tmqC2DEnrAwcAGxFVoQ4GFgZWAb5ve3VgMnCEpDHAKcAetjcAzgBOrIY6Dni37XWAw7reDoDt\n",
       "W4mAws/aXs/2X4EXJa1THTcR+Ekz3k8qtyRJkmGKa36ZaHz6NK1vfPoe4GLbr9p+hejovQXwz0op\n",
       "AZxdHbcKsAbwG0m/JwJUlq2OuQc4R9K+wNRZzNXdHXo6MFHSKGBv4JxmvJlUbkmSJMMY1zwFOBC4\n",
       "lmtLh+kAAAmASURBVP9v795j5SjrMI5/H0tR7i2CmFCqiHIpAdIiV21cIIb7HaNiJCh/AMrFCAIh\n",
       "6HKighBFMUo1jUFtBBJBG8HSIrdYSkHojVIuAQIJFWO4y91efv4x78J0bXvmcHZnZqfPJ9l0d/bd\n",
       "2edM98zvzOw775tNfLpjv96KNYtOZ1n++z7l2i2LiMnptmdEdHqzHwn8EpgCPCBpzDrW23ETcDhw\n",
       "FPBgRLzcix/Gxc3MrObStXCXkI3yf4+GNLkPbzMXOE7SJpI2A45PyyZK2j+1OTktexzYtrNc0lhJ\n",
       "k9L1yRPTKFEXAVsBm3e9z2vAlu/+bBHvAHOAaWSnN3vCxc3MbEBEO64BzgbmaEif7+m6IxYBvwX+\n",
       "AdwHTAdeJitk35T0CFmxmhYRK4CTgCskLQYWAQcAY4AZkh4imwn86oh4lTWPAG8AviNpgfTuUeh1\n",
       "wGrgtl79PL4UwMxswGhIU4EbgW9HO/7wvtZRo32npPOBLSKi3bN1uriZmQ0eDWl3YBbwC+DHI70W\n",
       "ri77Tkl/BnYEDo6Il3q2Xhc3M7PBpCFNIJun7Q6yo7jVhV/b8H2nv3MzMxtQ0Y7lwFSy+S6v15A+\n",
       "WHGk2nBxMzMbYGni00PJOnP0feLTQeHiZmY24KIdbwNfBB4G5mpI2w/zksZzcTMza4BoxyrgHLJu\n",
       "9fM0pN0qjlQpdygxM2sYDekU4ErghGjHvWtt0/B9p4/czMwaJtrxe7JBkGdqSMdWHKcSLm5mZg0U\n",
       "7ZgNHAFM05BOrzpP2Xxa0syswTSkT5JNfHod0O5c7N30faeLm5lZw2lIHwH+CiwBzoh2rGz6vtOn\n",
       "Jc3MGi5NfHoQMIESJj6tAxc3M7MNQJr49GjgReDOiuP0nYubmdkGIk18eirZWJSN5uJmZrYBSROf\n",
       "Xry25ySdI+kRSTNG8x6S7pa091qW7y3p6vW87mOSvjya9+7YqBcrMTOzRjgTOCQinhvletbaUzEi\n",
       "FgAL1vO6Hclm+75+lO/vIzczMwNJvwI+AcyWdIGkeyUtlDRP0s6pzamSZkq6TdLTks6SdH5qN1/S\n",
       "+NwqvyDpfkmPS/psen1L0s3p/uckLUq3BZI2B34ETE3Lzh3Nz+PiZmZmRMQZwHNAC5gGTI2IKUAb\n",
       "uCzXdHfgeGAf4IfAf1K7+cApuXZjImI/4FtpHd3OA74REZPJpu15C7gQmBsRkyNinacvi3BxK4mk\n",
       "VtUZinDO3nLO3hqEnIOQsYBxwI2SlgJXAZNyz90VEW9ExAvAK8DNaflS4OO5dn9K/y7sWt4xD/ip\n",
       "pLOB8RGxCujZdXcubuVpVR2goFbVAQpqVR2goFbVAQpqVR2goFbVAQpoVR1glAR8H7gjIvYgu3xg\n",
       "k9zz7+Tur849Xs2a/Tg6y1exlv4dEXEFcFpa9zxJu/QkfeIOJWZm1m1LslOUAF8r+JoRHXVJ2iki\n",
       "lgHLJO0D7AIsB7YYyXrWxUduZmbWEel2JXC5pIVkM3xH1/P59t2vXdd6u++fK2mppCXAf4FbgYeA\n",
       "VZIWj7ZDSSPGlqw6g5nZIGry2JIDX9zMzMy6+bSkmZk1joubmZk1zsAUN0mHSXpM0hOSLlxHm5+n\n",
       "55dImlx2xpRhvTkl7Zqu5H9b0nlVZEw5hsv5lbQdH0ojFOxZ05zHppydUQ4OrlvGXLt9JK2UdEKZ\n",
       "+XLvP9y2bEl6NTdqxCV1zJnatFLGhyXdXXLETobhtuf5uW25NP3fj6thzm0kzU6dOB6WdGrZGfsi\n",
       "Imp/I+ut8yTZhYBjgcXAbl1tjgBmpfv7AffVNOe2wKeBHwDn1Xh7HgBsle4fVuPtuVnu/h7Ak3XL\n",
       "mGt3J3ALcGJNt2UL+EsVn8kR5hwHLAMmpMfb1DFnV/ujgNvrmBO4FLi8sy3JpsTZqMrPQS9ug3Lk\n",
       "ti/ZTuuZiFgB3AAc29XmGOB3ABFxPzBO0nblxhw+Z0Q8HxEPAitKzpZXJOf8iHg1PbyfbJLDshXJ\n",
       "+Ubu4ebACyXmg2KfTYCzgRuB58sMl1M0Z9W954rkPBm4KSKWA0Q2UkbZim7Pjp4MBvw+FMn5L7Lr\n",
       "2kj/vhgRK0vM2BeDUty2B57NPV6elg3XpuwdcpGcdTDSnKcBs/qaaO0K5ZR0nKRHya6TOaekbB3D\n",
       "ZpS0PdkOZVpaVEUX5SLbMoAD02neWZImUb4iOT8FbC3pLkkPSvpqaeneU/h3SNKmwKHATSXk6lYk\n",
       "53Rgd0nPAUuAUV1fVheDMkJJ0Z1B91+dZe9EBuW6isI5JR0EfB34TP/irFOhnBExE5gpaSowg2yk\n",
       "g7IUyfgz4KKICEmimqOjIjkXAjtExJuSDgdmAjv3N9b/KZJzLDAFOATYFJgv6b6IeKKvydY0kt/1\n",
       "o4F7IuKVfoVZjyI5LwYWR0RL0k7A3yTtFRGv9TlbXw3Kkds/gR1yj3cg+wtkfW0mpGVlKpKzDgrl\n",
       "TJ1IpgPHRMTLJWXLG9H2jIi5wEaSPtzvYDlFMu4N3CDpaeBE4BpJx5SUr2PYnBHxWkS8me7fCoyV\n",
       "tHV5EYFi2/NZ4LaIeCsiXgT+DuxVUr6OkXw2v0Q1pyShWM4DgT8CRMRTwNOU+wdif1T9pV+RG9kR\n",
       "5lNkX4puzPAdSvanmg4Qw+bMtb2U6jqUFNmeE8m+iN6/5v/vO/HeYARTgKfqlrGr/bXACTXdltvl\n",
       "tuW+wDM1zbkrcDtZZ4lNyUajn1S3nKndVmQdNDYpe1uOYHteBbRzn4HlwNZV5O3lbSBOS0bESkln\n",
       "AXPIPtC/iYhHJZ2env91RMySdISkJ4E3KD7YZ6k5JX0UeIDsi9vVafy0SRHxep1yAt8DxgPTsjNp\n",
       "rIiIfcvKOIKcJwKnSFoBvE72V3LdMlauYM6TgDMlrQTepORtWTRnRDwmaTbZOISrgekR8Ujdcqam\n",
       "xwFzIuKtMvONMOdlwLXKxnj8AHBBRLxURd5e8vBbZmbWOIPynZuZmVlhLm5mZtY4Lm5mZtY4Lm5m\n",
       "ZtY4Lm5mZtY4Lm5mZtY4Lm5mZtY4Lm5mZtY4Lm5mfSBpvKTrKhib0cxwcTPri8gGmr6TbEgrMyuZ\n",
       "i5tZ/9zM+iewNLM+cXEz65OI+DewmaQth21sZj3l4mbWJ5I+RDZTwZFVZzHb0Li4mfWBpDFkc/Z9\n",
       "l2zaEzMrkYubWX/8BJgREYuAiZI2rjqQ2YbExc2sxySdBCyIiGVp0S1kM8WbWUk8WamZmTWOj9zM\n",
       "zKxxXNzMzKxxXNzMzKxxXNzMzKxxXNzMzKxxXNzMzKxxXNzMzKxxXNzMzKxx/gdilRClZUUnZQAA\n",
       "AABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe775080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos a probar la función\n",
    "A = lasso_logit_path(yy,fullxmat,0.8)\n",
    "grd_lamb = np.linspace(0.0001, 0.8, 10)\n",
    "df = pd.DataFrame(A, index = grd_lamb, columns=labs_x)\n",
    "ax = df.plot()\n",
    "#plt.axis([])\n",
    "plt.grid('off')\n",
    "ax.legend_.remove()\n",
    "plt.axis([0,0.8,-0.35,0.62])\n",
    "ax.set_title('Path Completo Lasso Logit')\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('Coeficientes')\n",
    "# Obtengamos las etiquetas\n",
    "for v in range(9):\n",
    "    lab_v = labs_x[v]\n",
    "    plt.text(0.81,df[lab_v].iloc[-1], lab_v, horizontalalignment='left', verticalalignment='top')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Comparémoslo con los resultados del libro\n",
    "\n",
    "\n",
    "![alt text](figures/lasso_logit.png)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
