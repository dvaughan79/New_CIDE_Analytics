{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analítica y Ciencia de Datos\n",
    "\n",
    "## CIDE - Otoño 2015\n",
    "\n",
    "### Solución Examen 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. ¿Qué es el Teorema de Frisch-Waugh (Lovell) y para qué sirve?  De un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El teorema de FWL dice que el coeficiente (o coeficientes) estimado por OLS de una (s) variable(s) se obtiene mediante “partialling-out” o controlando por las demás variables.  Así, para obtener un coeficiente, podríamos utilizar el siguiente método en tres etapas:\n",
    "\n",
    "a.\tObtenemos los residuos de $y$ con las demás variables.\n",
    "\n",
    "b.\tObtenemos los residuos del regresor o regresores que nos interesa con las demás variables.\n",
    "\n",
    "c.\tHacemos una regresión de los residuos en (a) en los residuos de (b).  Los coeficientes estimados son exactamente los mismos que obtendríamos por OLS.\n",
    "    \n",
    "El teorema sirve para interpretar los coeficientes como efectos marginales *netos* de otras variables.\n",
    "\n",
    "Un ejemplo es en  series de tiempo: $y_t = a + b X_t + ct + \\epsilon_t$.  Si $y_t$ y $X_t$ tienen una tendencia determinística, al incluir la tendencia t estamos limpiando simultáneamenate a $y$ y $X$  de la tendencia, así que evitamos tener una correlación espúrea sólo porque han presentado crecimientos similares.\n",
    "\n",
    "\n",
    "**Cómo lo califiqué**:\n",
    "\n",
    "* 1 punto: el teorema en la versión completa o de dos variables que di en las Notas de Clase.\n",
    "\n",
    "* 0.5 pto: para qué sirve?  Para entender que los coeficientes de OLS se deben interpretar como efectos marginales netos de otras variables.\n",
    "\n",
    "* 0.5 pto: un ejemplo práctico como el de arriba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\t¿Qué es el “trade-off” entre sesgo y varianza, y por qué nos importa?  De un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trade-off de varianza y sesgo se obtiene al descomponer el MSE entre la varianza de la estimación y el sesgo (al cuadrado).  La varianza y el sesgo se obtienen cuando utilizamos diferentes muestras de datos para estimar, así que la varianza nos dice qué tanto cambia una estimación con diferentes muestras, y el sesgo  a la diferencia sistemática entre el valor esperado o promedio de un estimador utilizando diferentes muestras y el estimador verdadero.\n",
    "\n",
    "Nos importa por lo siguiente: en algunas ocasiones querremos tener un estimador sesgado pero con menor varianza, si el poder predictivo en muestras de prueba o validación es mayor.\n",
    "\n",
    "Un ejemplo es el ajuste de una regresión bivariada con polinomios de la variable independiente.  Podríamos tener un menor sesgo si incluimos polinomios de orden cada vez más alto, pero podemos incurrir en “overfitting” y así aumentar la varianza.\n",
    "\n",
    "**Cómo lo califiqué**:\n",
    "\n",
    "* 1 punto: explicación \"formal\" del trade-off, ya sea mostrando alguna versión del teorema, como la de las Notas de Clase, o explicando como lo hago arriba.\n",
    "* 0.5 pto: para qué sirve? \n",
    "* 0.5 pto: un ejemplo práctico como el de arriba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tEn clase hablamos de tres tipos de errores de programación: (i) sintácticos, (ii) run time, (iii) semánticos.  Escriba el ejemplo más simple que pueda de cada uno, utilizando el lenguaje de programación Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Errores sintácticos**: la instrucción que escribimos en un código no está en las reglas sintácticas del lenguaje.  Por ejemplo: a(3,3) = 1.  En Python debemos utilizar paréntesis cuadrados, así que obtenemos un error sintáctico.\n",
    "\n",
    "**Errores de runtime**: un error que no es sintáctico, es decir, el lenguaje compila el código, se da cuenta que está bien escrito bajo las reglas sintácticas del mismo, y procede a ejecutarlo pero no puede terminar su ejecución.  Por ejemplo: a = b+2.  La variable b no ha sido declarada, así que no puede proceder, aunque la sintaxis es correcta.\n",
    "\n",
    "**Errores semánticos**: desde el punto de vista del lenguaje no hay ningún error visible.  La ejecución termina y obtenemos el resultado.  El problema es que no es el resultado correcto, simplemente porque pedimos que hiciera algo distinto a lo que queríamos.  Ejemplo: beta = np.dot(np.dot(x.T,x),np.dot(x.T,y)).  Se nos olvidó invertir la matriz de productos cruzados de X.\n",
    "\n",
    "**cómo califiqué**\n",
    "\n",
    "Cada error vale lo mismo: 2/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tEn clase, cuando hablamos de visualización,  mencionamos el “ink-data” ratio.  ¿Qué es, y cómo lo podemos utilizar como criterio para generar mejore visualizaciones?  De un ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **ink-data ratio** nos dice cuánta tinta utilizamos en una visualización, en relación a los datos que queríamos visualizar.  El objetivo es minimizar el ratio, sujeto a que no perdamos información que queríamos visualizar.  Ejemplo: barras coloreadas o con texturas.  La tinta adicional no da información adicional.  Se hace puramente por razones estéticas pero puede contaminar la interpretación visual de los datos.\n",
    "\n",
    "**Cómo lo califiqué**:\n",
    "\n",
    "* Qué es: 1pto\n",
    "* Uso: 0.5pto\n",
    "* Ej: 0.5pto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\t¿Cuáles son las diferencias entre los métodos de Monte Carlo, Bootstrapping, Jackknife y Cross-Validation?  Explique para que sirve cada uno, y muestre con un pseudocódigo su ejecución.   NOTA: el pseudocódigo no puede tener más de cinco líneas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo**: un experimento de MC se utiliza para verificar computacionalmente las propiedades asintóticas de un estimador, como sesgo y varianza, o poder y tamaño de una prueba estadística.  Se utiliza simulando distintas muestras de observaciones, que en la práctica se hace simulando distintos términos de error o residuos.\n",
    "\n",
    "*Pseudocódigo*:\n",
    "\n",
    "> *\tGeneremos un modelo, con parámetros beta, regresores X, y proceso de generación de datos f(X). \n",
    "> *\tRepetimos K veces lo siguiente\n",
    "    a.\tSimulamos los residuos $\\epsilon_{(k)}$, y por esta vía $y_{(k)}$.\n",
    "    b.\tEstimamos $\\beta_{(k)}$ y cualquier otro momento que nos interese.  Y lo guardamos.\n",
    "> * Obtenemos el estimador muestral del momento que queremos y concluimos.\n",
    "\n",
    "**Bootstrapping**: es un método para estimar errores estándar, o cualquier otro momento que se pueda obtener de la distribución muestral del estimador.  Consiste en obtener B muestras con reemplazo de las observaciones, estimar los coeficientes o función de los coeficientes para cada uno, y agregar utilizando la distribución muestral.\n",
    "\n",
    "*Pseudocódigo*:\n",
    "\n",
    "1.\tVamos a repetir B veces lo siguiente:\n",
    "a.\tObtener una muestra de tamaño N con remplazo de las obsevaciones.\n",
    "b.\tEstimar los coeficientes o función de coeficientes y guardar.\n",
    "2.\tObtener la distribución muestral de los coeficientes o función de los coeficientes.\n",
    "3.\tAgregar utilizando la distribución muestral.\n",
    "\n",
    "\n",
    "**Jackknife**: similar a Bootstrapping, consistente en estimar N veces los coeficientes o funciones de los coeficientes, cada vez excluyendo de la estimación una observación.  Al final se agrega utilizando la distribución muestral de los coeficientes o función de los coeficientes.\n",
    "\n",
    "*Pseudocódigo*:\n",
    "\n",
    "1.\tVamos a repetir  N veces lo siguiente:\n",
    "a.\tExcluir la observación i del análisis.\n",
    "b.\tEstimar los coeficientes o función de los coeficientes y guardar.\n",
    "c.\tSeguir con el siguiente i.\n",
    "2.\tAgregar con la distribución muetral.\n",
    "\n",
    "\n",
    "**Cross-Validation**:  es un método que  nos permite estimar el error de predicción por fuera de la muestra de entrenamiento, reduciendo la varianza de la estimación del error.  La versión que vimos en clase es k-fold cross-validation que consiste en dividir en K grupos de tamaño similar y de manera aleatoria la muestra total.  Se excluye un grupo sucesivamente, se utilizan los restantes como muestra de entrenamiento, y se obtiene el error de predicción utilizando el grupo excluido como muestra de validación.  Al final se promedian los errores de predicción.\n",
    "\n",
    "*Pseudocódigo*:\n",
    "\n",
    "1. Se divide la muestra en K grupos de manera aleatoria.\n",
    "2. Se repite el siguiente proceso K veces:\n",
    "\ta. Excuimos el grupo k, y utilizamos los siguientes como muestra de entrenamiento y estimamos los coeficientes.\n",
    "\tb. Estimamos el error de predicción con los coeficientes del paso anterior y la muestra de validación, es decir, el grupo k.  Guardamos.\n",
    "3. Promediamos los errores de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cómo lo califiqué**\n",
    "\n",
    "1. Cada método vale 0.5 ptos.\n",
    "2. Corté los pseudocódigos a las primeras cinco líneas.  Cualquier instrucción después fue obviada.\n",
    "3. La explicación correcta vale 0.3 ptos.\n",
    "4. El pseudocódigo 0.2ptos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
