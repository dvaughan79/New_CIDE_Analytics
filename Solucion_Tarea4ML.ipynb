{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analítica y Ciencia de Datos\n",
    "\n",
    "## CIDE - Otoño 2015\n",
    "\n",
    "### SOLUCION: Tarea 4: Introducción al Aprendizaje Estadístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Necesitamos nuestra función de OLS.\n",
    "# vamos a crear una función OLS para utilizar varias veces\n",
    "def olsfun(y,X):\n",
    "    '''\n",
    "    ------------------\n",
    "    Objetivo: estimar los coeficientes de un modelo de regresión lineal por medio de mínimos cuadrados\n",
    "    ordinarios (OLS).\n",
    "    ------------------\n",
    "    Argumentos: \n",
    "    y: Variable dependiente: de tamaño Nx1\n",
    "    X: matriz de variables independientes: de tamaño NxK\n",
    "    ------------------\n",
    "    Returns:\n",
    "    betahat: vector de coeficientes estimados\n",
    "    mse    : error cuadrático medio muestral\n",
    "    vcv    : matriz de varianzas y covarianzas de los coeficientes\n",
    "    yhat   : y estimado\n",
    "    ------------------\n",
    "    '''\n",
    "    N,K = X.shape\n",
    "    # necesitamos la matriz inversa\n",
    "    xtx = np.dot(X.T, X)\n",
    "    xtxinv = np.linalg.inv(xtx)\n",
    "    xty = np.dot(X.T,y)\n",
    "    betahat = np.dot(xtxinv,xty)\n",
    "    # obtengamos también el MSE\n",
    "    yhat = np.dot(X,betahat)\n",
    "    resids = y-yhat\n",
    "    ssr  = np.dot(resids.T,resids)\n",
    "    s2   = ssr/(N-K)\n",
    "    mse  = ssr/N\n",
    "    # la matriz de VCV\n",
    "    vcv  = s2*xtxinv \n",
    "    return betahat, mse[0,0], vcv, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 1: Método Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 1. Loglinealícelo**:\n",
    "\n",
    "Lo hicimos en las notas:\n",
    "\n",
    "$$\n",
    "\\ln(y) = \\beta_0 + \\beta_1 \\ln(x) + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 2. Qué parámetros podemos estimar:**\n",
    "\n",
    "El modelo es lineal en $\\beta_0$ y $\\beta_1$, así que los podemos estimar con OLS.  Los parámetros profundos $\\alpha_0$ y $\\alpha_1$ no se pueden estimar directamente.\n",
    "\n",
    "Una vez tengamos $\\hat{\\beta_i}$, podemos obtener $\\hat{\\alpha_i}$ mediante las transformacinoes correspondientes:\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\ln(\\alpha_0)  \\Longrightarrow \\alpha_0 = e^{\\beta_0}\\\\\n",
    "\\beta_1 = 1/\\alpha_1     \\Longrightarrow \\alpha_1 = 1/\\beta_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 3: Método Delta**\n",
    "\n",
    "Como vimos en las Notas:\n",
    "\n",
    "Si $\\alpha = f(\\beta)$, y $\\beta \\sim N(\\mu_b,\\sigma_b^2)$\n",
    "\n",
    "$$\n",
    "f(\\beta) \\approx f(\\beta_0) + f'(\\beta_0)(\\beta - \\beta_0)\n",
    "$$\n",
    "así que, si denotamos a $\\alpha_0 := f(\\beta_0)$\n",
    "$$\n",
    "\\alpha - \\alpha_0 \\sim^{d} N(0, f'(\\beta_0)^2 \\sigma_b^2)\n",
    "$$\n",
    "\n",
    "(para los interesados $\\sim^d$ significa convergencia en distribución, aunque una explicación de esto nos lleva por fuera del tema del curso)\n",
    "\n",
    "* En nuestro caso: $\\alpha_1 = f(\\beta_1) = 1/\\beta_1$, así que\n",
    "\n",
    "$$\n",
    "\\alpha_1 - \\alpha^*_{1} \\sim^d N(0, (1/\\beta_1)^2 \\sigma_{b1}^2)\n",
    "$$\n",
    "\n",
    "donde las $*$ denotan al parámetro verdadero.\n",
    "\n",
    "* Y para la constante: $\\alpha_0 = e^{\\beta_0}$, así que\n",
    "\n",
    "$$\n",
    "\\alpha_0 - \\alpha^*_{0} \\sim^d N(0,  e^{2\\beta_0} \\sigma_{b0}^2)\n",
    "$$\n",
    "\n",
    "Hasta aquí la teoría.  En la práctica el procedimiento es trivial:\n",
    "\n",
    "1. Estimar el modelo loglineal, y obtener las varianzas de los parámetros $\\hat{\\beta}_0, \\hat{\\beta}_1$.\n",
    "2. Por lo anterior sabemos que:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Var(\\hat{\\alpha}_0) &=& e^{2\\hat{\\beta}_0} \\hat{\\sigma}_{b0}^2\\\\\n",
    "Var(\\hat{\\alpha}_1) &=& (1/\\hat{\\beta}_1)^2 \\hat{\\sigma}_{b1}^2\n",
    "\\end{eqnarray*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 4: Simule el modelo y estime las varianzas del punto anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.21867885]\n",
      " [ 0.46153297]]\n",
      "[[ 0.03523143  0.01086236]\n",
      " [ 0.01086236  0.00376306]]\n",
      "----------------\n",
      "alpha0 verdadero y el estimado son:  (10, 9.1951746326739574)\n",
      "alpha1 verdadero y el estimado son:  (2.0, 2.1666924372982828)\n",
      "----------------\n",
      "la varianza de alpha0 =  2.97886075138\n",
      "la varianza de alpha1 =  0.0176659180991\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(19790625)\n",
    "N = 300\n",
    "alpha0 = 10\n",
    "alpha1 = 2.0\n",
    "# para que sea positivo, desplacémoslo\n",
    "x      = 0.1*np.random.randn(N,1)\n",
    "epsilon = np.sqrt(0.5)*np.random.randn(N,1)\n",
    "# listos para estimar el modelo:\n",
    "beta1  = 1/alpha1\n",
    "y = alpha0*np.power(x,beta1)*np.exp(epsilon)\n",
    "# Eliminemos los NaNs que se obtendrán cuando hagamos ln(x)\n",
    "data = pd.DataFrame(np.concatenate((y,np.log(x)),axis=1), columns=['y','x']).dropna(subset= ['x'])\n",
    "N = data.shape[0]\n",
    "y = np.asarray(data.y).reshape((N,1))\n",
    "x = np.asarray(np.exp(data.x.values)).reshape((N,1))\n",
    "# El modelo a estimar loglineal:\n",
    "lny = np.log(y)\n",
    "lnx = np.log(x)\n",
    "xmat = np.concatenate((np.ones((N,1)),lnx),axis=1)\n",
    "# estimemos\n",
    "resols = olsfun(lny,xmat)\n",
    "# Nos interesan las varianzas\n",
    "betahat = resols[0]\n",
    "vcv     = resols[2]\n",
    "print betahat\n",
    "print vcv\n",
    "# Obtengamos los parámetros estructurales:\n",
    "alpha0hat = np.exp(betahat[0])\n",
    "alpha1hat = 1/betahat[1]\n",
    "print \"----------------\"\n",
    "print \"alpha0 verdadero y el estimado son: \", (alpha0,alpha0hat[0])\n",
    "print \"alpha1 verdadero y el estimado son: \", (alpha1,alpha1hat[0])\n",
    "print \"----------------\"\n",
    "# finalmente, las varianzas:\n",
    "var_a0 = np.exp(2*betahat[0][0])*vcv[0,0]\n",
    "var_a1 = ((1/betahat[1][0])**2)*vcv[1,1]\n",
    "print \"la varianza de alpha0 = \", var_a0\n",
    "print \"la varianza de alpha1 = \", var_a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 5: comparación con Bootstrapping.**\n",
    "\n",
    "En las notas de clase vimos que \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Var(\\hat{\\alpha}_0^2) &=& 0.00389043\\\\\n",
    "Var(\\hat{\\alpha}_1^2) &=& 0.09927349\\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "La varianza de $\\alpha_1$ son comparables en términos de órdenes de magnitud, pero no es así en el caso de $\\alpha_0$, donde bootstrapping produce una varianza considerablemente más baja.\n",
    "\n",
    "Lo importante no es la varianza, per sé, sino los efectos que esto tiene sobre los resultados de las pruebas de hipótesis.\n",
    "\n",
    "En particular, queremos que, en promedio, los dos métodos permitan rechazar la hipótesis nula de ausencia de los efectos con la misma probabilidad, pero esto sólo podríamos saberlo con un experimento de Monte Carlo, en donde simulemos el modelo $K$ veces, y obtengamos la fracción de veces en que los correspondientes p-values permiten rechazar $H_0$.  Sólo así podríamos saber si los métodos producen resultados comparables y cuál es mejor o peor.\n",
    "\n",
    "No obstante, lo que sí podemos decir ya es lo siguiente:\n",
    "\n",
    "> El Método Delta hace una aproximación de Taylor de primer orden, o lineal, así que la calidad de la misma depende de qué tan no lineal es el problema.  Este no es el caso con Bootstrapping, así que en general, esta técnica es preferible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2: Jackknife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en las notas de clase, tenemos que hacer N estimaciones, excluyendo cada observación\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|l}\n",
    "iter & muestra \\\\\n",
    "\\hline\n",
    "0    & 1,2,\\cdots,N-1 \\\\\n",
    "1    & 0,2,\\cdots,N-1 \\\\\n",
    "2    & 0,1,3,\\cdots,N-1 \\\\\n",
    "\\vdots    & \\vdots \\\\\n",
    "N-1  & 0,1,2,\\cdots, N-2\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.72414413e-04   3.05673761e-05]\n"
     ]
    }
   ],
   "source": [
    "# Inicialicemos donde vamos a guardar los resultados\n",
    "JKEstimator = np.zeros((N,2))\n",
    "# podemos empezar la iteración\n",
    "for i in range(N):\n",
    "    # para excluir la i-ésima observación podemos utilizar la función setdiff1d, por ejemplo\n",
    "    ind_inc = np.setdiff1d(np.arange(N), np.array([i]))\n",
    "    # listo para seleccionar los datos:\n",
    "    y_i = np.log(y[ind_inc]).reshape((N-1,1))\n",
    "    x_i = np.log(x[ind_inc]).reshape((N-1,1))\n",
    "    # Estimemos el modelo\n",
    "    olsi = olsfun(y_i, np.concatenate((np.ones((N-1,1)),x_i),axis=1))\n",
    "    betai = olsi[0].flatten()\n",
    "    # Guardemos los resultados:\n",
    "    JKEstimator[i,:] = betai\n",
    "# Listos, obtengamos la varianza:\n",
    "JKVar = np.var(JKEstimator,axis=0)\n",
    "print JKVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La varianza es aún menor que la que estimamos con Bootstrapping: [ 0.00389043  0.09927349]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 3: Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24\n",
       "2    20\n",
       "3    18\n",
       "4    25\n",
       "5    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Es muy simple:\n",
    "N = 100\n",
    "grp_ind = np.dot(np.random.multinomial(1,[1.0/5]*5,N),np.arange(1,6).reshape(5,1)).flatten()\n",
    "# Revisemos que quedó bien hecho con un groupby\n",
    "pd.Series(grp_ind).groupby(pd.Series(grp_ind)).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
